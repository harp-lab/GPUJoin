## Intermediate result

```shell
nvcc transitive_closure.cu -run -o join
```

Benchmark for SF.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 258,428 | 223,001 | 0.0018 | 0.0039 | 0.0001 | 0.0005 |
| 2 | 297,179 | 476,246 | 0.0019 | 0.0042 | 0.0003 | 0.0007 |
| 3 | 340,027 | 755,982 | 0.0020 | 0.0046 | 0.0004 | 0.0009 |
| 4 | 388,465 | 1,056,726 | 0.0021 | 0.0053 | 0.0005 | 0.0012 |
| 5 | 443,213 | 1,376,010 | 0.0022 | 0.0075 | 0.0005 | 0.0014 |
| 6 | 504,535 | 1,711,750 | 0.0021 | 0.0063 | 0.0007 | 0.0017 |
| 7 | 573,556 | 2,062,150 | 0.0022 | 0.0070 | 0.0006 | 0.0019 |
| 8 | 650,327 | 2,426,401 | 0.0035 | 0.0077 | 0.0008 | 0.0022 |
| 9 | 735,590 | 2,803,627 | 0.0027 | 0.0082 | 0.0009 | 0.0024 |
| 10 | 829,452 | 3,193,373 | 0.0026 | 0.0088 | 0.0008 | 0.0026 |
| 11 | 932,759 | 3,595,385 | 0.0027 | 0.0094 | 0.0010 | 0.0033 |
| 12 | 1,045,446 | 4,009,951 | 0.0028 | 0.0102 | 0.0013 | 0.0035 |
| 13 | 1,167,469 | 4,437,031 | 0.0030 | 0.0121 | 0.0014 | 0.0036 |
| 14 | 1,298,788 | 4,876,068 | 0.0031 | 0.0113 | 0.0016 | 0.0043 |
| 15 | 1,439,204 | 5,326,697 | 0.0033 | 0.0120 | 0.0016 | 0.0043 |
| 16 | 1,588,293 | 5,788,461 | 0.0085 | 0.0171 | 0.0017 | 0.0049 |
| 17 | 1,745,552 | 6,260,652 | 0.0034 | 0.0141 | 0.0019 | 0.0053 |
| 18 | 1,910,098 | 6,742,695 | 0.0036 | 0.0148 | 0.0018 | 0.0056 |
| 19 | 2,082,226 | 7,233,796 | 0.0039 | 0.0198 | 0.0022 | 0.0063 |
| 20 | 2,261,690 | 7,733,688 | 0.0042 | 0.0169 | 0.0022 | 0.0067 |
| 21 | 2,448,444 | 8,241,532 | 0.0043 | 0.0178 | 0.0026 | 0.0072 |
| 22 | 2,642,134 | 8,756,848 | 0.0046 | 0.0190 | 0.0026 | 0.0074 |
| 23 | 2,842,783 | 9,279,010 | 0.0110 | 0.0236 | 0.0028 | 0.0082 |
| 24 | 3,049,532 | 9,807,603 | 0.0051 | 0.0209 | 0.0031 | 0.0083 |
| 25 | 3,262,011 | 10,341,798 | 0.0053 | 0.0218 | 0.0029 | 0.0092 |
| 26 | 3,479,859 | 10,881,222 | 0.0057 | 0.0226 | 0.0031 | 0.0098 |
| 27 | 3,702,563 | 11,425,295 | 0.0059 | 0.0239 | 0.0037 | 0.0096 |
| 28 | 3,930,141 | 11,973,638 | 0.0062 | 0.0251 | 0.0039 | 0.0103 |
| 29 | 4,162,826 | 12,526,796 | 0.0063 | 0.0259 | 0.0040 | 0.0108 |
| 30 | 4,399,940 | 13,084,624 | 0.0068 | 0.0304 | 0.0044 | 0.0109 |
| 31 | 4,639,830 | 13,646,848 | 0.0067 | 0.0277 | 0.0042 | 0.0120 |
| 32 | 4,881,763 | 14,212,245 | 0.0071 | 0.0293 | 0.0049 | 0.0126 |
| 33 | 5,125,489 | 14,779,678 | 0.0072 | 0.0323 | 0.0054 | 0.0126 |
| 34 | 5,370,961 | 15,348,483 | 0.0073 | 0.0314 | 0.0056 | 0.0138 |
| 35 | 5,617,281 | 15,918,265 | 0.0077 | 0.0320 | 0.0057 | 0.0139 |
| 36 | 5,863,799 | 16,488,145 | 0.0080 | 0.0333 | 0.0062 | 0.0142 |
| 37 | 6,110,246 | 17,057,403 | 0.0088 | 0.0342 | 0.0062 | 0.0151 |
| 38 | 6,356,681 | 17,625,807 | 0.0084 | 0.0358 | 0.0065 | 0.0154 |
| 39 | 6,602,554 | 18,193,301 | 0.0083 | 0.0362 | 0.0069 | 0.0168 |
| 40 | 6,847,726 | 18,759,980 | 0.0092 | 0.0387 | 0.0073 | 0.0169 |
| 41 | 7,092,498 | 19,325,886 | 0.0093 | 0.0386 | 0.0075 | 0.0172 |
| 42 | 7,336,065 | 19,890,746 | 0.0096 | 0.0390 | 0.0083 | 0.0175 |
| 43 | 7,578,681 | 20,454,047 | 0.0103 | 0.0407 | 0.0087 | 0.0183 |
| 44 | 7,820,628 | 21,015,866 | 0.0103 | 0.0422 | 0.0083 | 0.0193 |
| 45 | 8,061,532 | 21,576,436 | 0.0103 | 0.0427 | 0.0087 | 0.0199 |
| 46 | 8,302,092 | 22,135,333 | 0.0107 | 0.0463 | 0.0090 | 0.0194 |
| 47 | 8,542,202 | 22,693,109 | 0.0119 | 0.0479 | 0.0098 | 0.0205 |
| 48 | 8,781,537 | 23,249,591 | 0.0110 | 0.0462 | 0.0100 | 0.0212 |
| 49 | 9,019,517 | 23,804,594 | 0.0114 | 0.0542 | 0.0088 | 0.0211 |
| 50 | 9,255,908 | 24,357,862 | 0.0115 | 0.0490 | 0.0108 | 0.0215 |
| 51 | 9,490,710 | 24,909,212 | 0.0123 | 0.0499 | 0.0102 | 0.0232 |
| 52 | 9,724,827 | 25,458,554 | 0.0124 | 0.0501 | 0.0105 | 0.0236 |
| 53 | 9,957,993 | 26,006,351 | 0.0124 | 0.0518 | 0.0118 | 0.0240 |
| 54 | 10,189,846 | 26,552,384 | 0.0119 | 0.0529 | 0.0118 | 0.0232 |
| 55 | 10,419,830 | 27,096,892 | 0.0129 | 0.0536 | 0.0117 | 0.0248 |
| 56 | 10,647,646 | 27,639,482 | 0.0131 | 0.0539 | 0.0121 | 0.0244 |
| 57 | 10,873,097 | 28,179,796 | 0.0131 | 0.0552 | 0.0121 | 0.0263 |
| 58 | 11,096,484 | 28,717,697 | 0.0181 | 0.0569 | 0.0125 | 0.0256 |
| 59 | 11,318,223 | 29,253,290 | 0.0132 | 0.0572 | 0.0128 | 0.0258 |
| 60 | 11,538,193 | 29,786,407 | 0.0137 | 0.0591 | 0.0129 | 0.0271 |
| 61 | 11,755,907 | 30,316,892 | 0.0138 | 0.0593 | 0.0131 | 0.0278 |
| 62 | 11,970,992 | 30,844,793 | 0.0147 | 0.0603 | 0.0134 | 0.0280 |
| 63 | 12,183,468 | 31,369,749 | 0.0139 | 0.0623 | 0.0138 | 0.0285 |
| 64 | 12,393,580 | 31,891,466 | 0.0147 | 0.0642 | 0.0143 | 0.0288 |
| 65 | 12,601,396 | 32,409,890 | 0.0142 | 0.0632 | 0.0142 | 0.0298 |
| 66 | 12,806,514 | 32,925,490 | 0.0147 | 0.0636 | 0.0149 | 0.0306 |
| 67 | 13,008,994 | 33,437,622 | 0.0153 | 0.0649 | 0.0150 | 0.0300 |
| 68 | 13,209,201 | 33,946,228 | 0.0159 | 0.0659 | 0.0151 | 0.0313 |
| 69 | 13,406,759 | 34,451,545 | 0.0155 | 0.0686 | 0.0154 | 0.0306 |
| 70 | 13,601,867 | 34,953,666 | 0.0167 | 0.0688 | 0.0159 | 0.0334 |
| 71 | 13,794,776 | 35,452,811 | 0.0159 | 0.0711 | 0.0158 | 0.0322 |
| 72 | 13,985,408 | 35,949,111 | 0.0175 | 0.0695 | 0.0163 | 0.0324 |
| 73 | 14,173,393 | 36,442,445 | 0.0166 | 0.0702 | 0.0163 | 0.0332 |
| 74 | 14,358,843 | 36,932,782 | 0.0167 | 0.0723 | 0.0170 | 0.0334 |
| 75 | 14,542,560 | 37,420,101 | 0.0193 | 0.0734 | 0.0167 | 0.0342 |
| 76 | 14,724,948 | 37,904,568 | 0.0172 | 0.0728 | 0.0171 | 0.0344 |
| 77 | 14,905,832 | 38,386,437 | 0.0173 | 0.0744 | 0.0172 | 0.0346 |
| 78 | 15,085,380 | 38,865,648 | 0.0170 | 0.0747 | 0.0177 | 0.0342 |
| 79 | 15,263,368 | 39,341,838 | 0.0193 | 0.0762 | 0.0179 | 0.0354 |
| 80 | 15,439,408 | 39,814,740 | 0.0172 | 0.0775 | 0.0181 | 0.0360 |
| 81 | 15,613,398 | 40,284,305 | 0.0171 | 0.0774 | 0.0185 | 0.0364 |
| 82 | 15,785,334 | 40,750,481 | 0.0181 | 0.0782 | 0.0188 | 0.0363 |
| 83 | 15,955,256 | 41,213,907 | 0.0183 | 0.0799 | 0.0187 | 0.0375 |
| 84 | 16,123,566 | 41,674,629 | 0.0182 | 0.0802 | 0.0194 | 0.0375 |
| 85 | 16,290,220 | 42,132,970 | 0.0179 | 0.0805 | 0.0191 | 0.0387 |
| 86 | 16,455,649 | 42,589,055 | 0.0183 | 0.0836 | 0.0194 | 0.0396 |
| 87 | 16,619,166 | 43,042,721 | 0.0186 | 0.0849 | 0.0192 | 0.0389 |
| 88 | 16,780,634 | 43,493,531 | 0.0191 | 0.0840 | 0.0188 | 0.0390 |
| 89 | 16,939,995 | 43,941,165 | 0.0187 | 0.0898 | 0.0194 | 0.0407 |
| 90 | 17,097,292 | 44,385,331 | 0.0191 | 0.0860 | 0.0200 | 0.0391 |
| 91 | 17,252,053 | 44,826,120 | 0.0191 | 0.0867 | 0.0202 | 0.0404 |
| 92 | 17,404,562 | 45,263,361 | 0.0194 | 0.0869 | 0.0206 | 0.0400 |
| 93 | 17,555,219 | 45,697,225 | 0.0195 | 0.0884 | 0.0208 | 0.0416 |
| 94 | 17,705,057 | 46,127,985 | 0.0196 | 0.0883 | 0.0207 | 0.0425 |
| 95 | 17,853,671 | 46,556,237 | 0.0198 | 0.0891 | 0.0212 | 0.0417 |
| 96 | 18,000,988 | 46,981,573 | 0.0207 | 0.0914 | 0.0205 | 0.0434 |
| 97 | 18,146,687 | 47,403,868 | 0.0270 | 0.0946 | 0.0207 | 0.0442 |
| 98 | 18,290,708 | 47,822,709 | 0.0214 | 0.0922 | 0.0216 | 0.0450 |
| 99 | 18,432,995 | 48,238,000 | 0.0205 | 0.0936 | 0.0214 | 0.0447 |
| 100 | 18,573,442 | 48,649,760 | 0.0208 | 0.0939 | 0.0217 | 0.0440 |
| 101 | 18,712,279 | 49,058,284 | 0.0212 | 0.0952 | 0.0221 | 0.0462 |
| 102 | 18,849,737 | 49,463,876 | 0.0206 | 0.0956 | 0.0223 | 0.0463 |
| 103 | 18,985,676 | 49,866,997 | 0.0209 | 0.0955 | 0.0220 | 0.0459 |
| 104 | 19,119,931 | 50,267,588 | 0.0213 | 0.0973 | 0.0222 | 0.0478 |
| 105 | 19,253,010 | 50,665,511 | 0.0215 | 0.0971 | 0.0222 | 0.0467 |
| 106 | 19,384,369 | 51,061,106 | 0.0211 | 0.0975 | 0.0220 | 0.0468 |
| 107 | 19,513,758 | 51,454,139 | 0.0218 | 0.0992 | 0.0226 | 0.0468 |
| 108 | 19,641,336 | 51,844,140 | 0.0215 | 0.0997 | 0.0233 | 0.0489 |
| 109 | 19,767,208 | 52,231,327 | 0.0209 | 0.1005 | 0.0226 | 0.0486 |
| 110 | 19,891,769 | 52,615,475 | 0.0211 | 0.1008 | 0.0232 | 0.0482 |
| 111 | 20,015,281 | 52,996,813 | 0.0215 | 0.1014 | 0.0231 | 0.0489 |
| 112 | 20,137,769 | 53,375,817 | 0.0218 | 0.1015 | 0.0238 | 0.0495 |
| 113 | 20,258,768 | 53,752,812 | 0.0217 | 0.1036 | 0.0236 | 0.0509 |
| 114 | 20,378,203 | 54,127,794 | 0.0223 | 0.1037 | 0.0239 | 0.0502 |
| 115 | 20,496,578 | 54,500,796 | 0.0222 | 0.1037 | 0.0243 | 0.0515 |
| 116 | 20,613,881 | 54,872,132 | 0.0226 | 0.1048 | 0.0241 | 0.0510 |
| 117 | 20,730,849 | 55,241,914 | 0.0225 | 0.1055 | 0.0249 | 0.0509 |
| 118 | 20,847,483 | 55,610,441 | 0.0226 | 0.1066 | 0.0243 | 0.0510 |
| 119 | 20,963,966 | 55,977,545 | 0.0222 | 0.1073 | 0.0251 | 0.0516 |
| 120 | 21,080,057 | 56,343,282 | 0.0224 | 0.1082 | 0.0243 | 0.0511 |
| 121 | 21,196,025 | 56,707,575 | 0.0225 | 0.1076 | 0.0246 | 0.0519 |
| 122 | 21,311,392 | 57,070,595 | 0.0230 | 0.1104 | 0.0255 | 0.0528 |
| 123 | 21,425,729 | 57,431,978 | 0.0240 | 0.1093 | 0.0250 | 0.0536 |
| 124 | 21,539,122 | 57,791,689 | 0.0230 | 0.1108 | 0.0255 | 0.0538 |
| 125 | 21,651,407 | 58,150,108 | 0.0229 | 0.1134 | 0.0259 | 0.0528 |
| 126 | 21,763,206 | 58,507,184 | 0.0241 | 0.1115 | 0.0255 | 0.0553 |
| 127 | 21,875,190 | 58,863,245 | 0.0235 | 0.1129 | 0.0260 | 0.0536 |
| 128 | 21,987,742 | 59,218,664 | 0.0235 | 0.1125 | 0.0259 | 0.0550 |
| 129 | 22,101,342 | 59,573,836 | 0.0237 | 0.1140 | 0.0259 | 0.0567 |
| 130 | 22,215,770 | 59,929,167 | 0.0237 | 0.1158 | 0.0260 | 0.0552 |
| 131 | 22,330,698 | 60,284,508 | 0.0242 | 0.1190 | 0.0221 | 0.0558 |
| 132 | 22,446,155 | 60,639,440 | 0.0238 | 0.1162 | 0.0266 | 0.0551 |
| 133 | 22,562,021 | 60,994,007 | 0.0245 | 0.1163 | 0.0267 | 0.0562 |
| 134 | 22,678,439 | 61,348,377 | 0.0244 | 0.1171 | 0.0268 | 0.0570 |
| 135 | 22,795,195 | 61,702,405 | 0.0243 | 0.1175 | 0.0269 | 0.0567 |
| 136 | 22,911,868 | 62,055,865 | 0.0250 | 0.1187 | 0.0270 | 0.0569 |
| 137 | 23,028,300 | 62,408,260 | 0.0279 | 0.1195 | 0.0270 | 0.0577 |
| 138 | 23,143,861 | 62,759,153 | 0.0253 | 0.1191 | 0.0277 | 0.0575 |
| 139 | 23,258,109 | 63,107,949 | 0.0250 | 0.1211 | 0.0276 | 0.0592 |
| 140 | 23,371,363 | 63,454,403 | 0.0247 | 0.1215 | 0.0281 | 0.0576 |
| 141 | 23,483,834 | 63,798,667 | 0.0252 | 0.1219 | 0.0279 | 0.0602 |
| 142 | 23,596,378 | 64,141,120 | 0.0253 | 0.1225 | 0.0285 | 0.0588 |
| 143 | 23,708,639 | 64,482,480 | 0.0344 | 0.1217 | 0.0285 | 0.0589 |
| 144 | 23,820,573 | 64,822,707 | 0.0260 | 0.1233 | 0.0287 | 0.0595 |
| 145 | 23,931,864 | 65,161,915 | 0.0252 | 0.1234 | 0.0282 | 0.0600 |
| 146 | 24,041,537 | 65,499,978 | 0.0254 | 0.1232 | 0.0287 | 0.0611 |
| 147 | 24,149,532 | 65,836,150 | 0.0260 | 0.1255 | 0.0286 | 0.0595 |
| 148 | 24,256,198 | 66,170,073 | 0.0263 | 0.1267 | 0.0290 | 0.0604 |
| 149 | 24,361,761 | 66,502,122 | 0.0260 | 0.1274 | 0.0294 | 0.0621 |
| 150 | 24,465,788 | 66,832,236 | 0.0263 | 0.1309 | 0.0288 | 0.0633 |
| 151 | 24,568,591 | 67,159,849 | 0.0259 | 0.1283 | 0.0298 | 0.0622 |
| 152 | 24,669,140 | 67,485,035 | 0.0261 | 0.1291 | 0.0289 | 0.0625 |
| 153 | 24,766,887 | 67,806,862 | 0.0260 | 0.1298 | 0.0299 | 0.0621 |
| 154 | 24,861,456 | 68,124,852 | 0.0259 | 0.1343 | 0.0260 | 0.0631 |
| 155 | 24,952,883 | 68,438,764 | 0.0264 | 0.1301 | 0.0306 | 0.0635 |
| 156 | 25,041,053 | 68,748,701 | 0.0265 | 0.1355 | 0.0270 | 0.0634 |
| 157 | 25,126,531 | 69,054,594 | 0.0269 | 0.1391 | 0.0300 | 0.0627 |
| 158 | 25,209,434 | 69,356,883 | 0.0266 | 0.1319 | 0.0307 | 0.0643 |
| 159 | 25,289,551 | 69,655,712 | 0.0273 | 0.1333 | 0.0302 | 0.0626 |
| 160 | 25,366,851 | 69,950,812 | 0.0275 | 0.1324 | 0.0310 | 0.0665 |
| 161 | 25,440,753 | 70,241,914 | 0.0269 | 0.1330 | 0.0310 | 0.0635 |
| 162 | 25,511,333 | 70,528,877 | 0.0274 | 0.1338 | 0.0308 | 0.0650 |
| 163 | 25,578,588 | 70,811,829 | 0.0272 | 0.1361 | 0.0295 | 0.0645 |
| 164 | 25,642,294 | 71,090,923 | 0.0284 | 0.1706 | 0.0312 | 0.0652 |
| 165 | 25,702,377 | 71,366,182 | 0.0591 | 0.1447 | 0.0490 | 0.0717 |
| 166 | 25,758,772 | 71,637,611 | 0.0278 | 0.1361 | 0.0314 | 0.0652 |
| 167 | 25,811,595 | 71,905,185 | 0.0280 | 0.1352 | 0.0316 | 0.0677 |
| 168 | 25,860,827 | 72,168,945 | 0.0272 | 0.1356 | 0.0313 | 0.0678 |
| 169 | 25,906,116 | 72,428,653 | 0.0281 | 0.1373 | 0.0310 | 0.0663 |
| 170 | 25,947,522 | 72,684,006 | 0.0277 | 0.1373 | 0.0315 | 0.0671 |
| 171 | 25,985,108 | 72,934,972 | 0.0278 | 0.1369 | 0.0313 | 0.0680 |
| 172 | 26,019,122 | 73,181,708 | 0.0280 | 0.1374 | 0.0316 | 0.0669 |
| 173 | 26,049,941 | 73,424,676 | 0.0274 | 0.1390 | 0.0314 | 0.0686 |
| 174 | 26,077,750 | 73,664,310 | 0.0276 | 0.1414 | 0.0314 | 0.0678 |
| 175 | 26,102,923 | 73,900,521 | 0.0280 | 0.1395 | 0.0317 | 0.0683 |
| 176 | 26,125,268 | 74,133,461 | 0.0281 | 0.1389 | 0.0316 | 0.0673 |
| 177 | 26,144,218 | 74,363,061 | 0.0362 | 0.1488 | 0.0285 | 0.0679 |
| 178 | 26,159,701 | 74,588,756 | 0.0283 | 0.1403 | 0.0314 | 0.0683 |
| 179 | 26,170,842 | 74,810,673 | 0.0282 | 0.1404 | 0.0323 | 0.0692 |
| 180 | 26,177,418 | 75,027,937 | 0.0274 | 0.1407 | 0.0313 | 0.0698 |
| 181 | 26,179,328 | 75,240,140 | 0.0285 | 0.1417 | 0.0318 | 0.0673 |
| 182 | 26,176,571 | 75,446,962 | 0.0281 | 0.1417 | 0.0316 | 0.0672 |
| 183 | 26,169,282 | 75,648,307 | 0.0276 | 0.1421 | 0.0318 | 0.0693 |
| 184 | 26,157,702 | 75,844,252 | 0.0280 | 0.1418 | 0.0321 | 0.0685 |
| 185 | 26,141,497 | 76,034,789 | 0.0281 | 0.1440 | 0.0318 | 0.0692 |
| 186 | 26,120,708 | 76,219,965 | 0.0280 | 0.1428 | 0.0320 | 0.0686 |
| 187 | 26,094,887 | 76,399,917 | 0.0275 | 0.1430 | 0.0312 | 0.0704 |
| 188 | 26,064,180 | 76,574,662 | 0.0282 | 0.1436 | 0.0317 | 0.0712 |
| 189 | 26,028,824 | 76,744,231 | 0.0280 | 0.1434 | 0.0315 | 0.0697 |
| 190 | 25,989,032 | 76,908,857 | 0.0282 | 0.1444 | 0.0319 | 0.0707 |
| 191 | 25,944,531 | 77,068,533 | 0.0279 | 0.1444 | 0.0314 | 0.0691 |
| 192 | 25,895,518 | 77,223,086 | 0.0277 | 0.1447 | 0.0312 | 0.0681 |
| 193 | 25,841,541 | 77,372,718 | 0.0273 | 0.1441 | 0.0316 | 0.0700 |
| 194 | 25,782,784 | 77,517,227 | 0.0275 | 0.1475 | 0.0311 | 0.0703 |
| 195 | 25,719,461 | 77,656,631 | 0.0267 | 0.1438 | 0.0314 | 0.0701 |
| 196 | 25,651,566 | 77,791,065 | 0.0269 | 0.1442 | 0.0312 | 0.0720 |
| 197 | 25,579,143 | 77,920,428 | 0.0270 | 0.1452 | 0.0314 | 0.0699 |
| 198 | 25,502,630 | 78,044,726 | 0.0261 | 0.1440 | 0.0301 | 0.0695 |
| 199 | 25,422,369 | 78,164,252 | 0.0277 | 0.1480 | 0.0318 | 0.0712 |
| 200 | 25,338,487 | 78,279,222 | 0.0270 | 0.1451 | 0.0311 | 0.0690 |
| 201 | 25,251,711 | 78,389,839 | 0.0265 | 0.1467 | 0.0302 | 0.0703 |
| 202 | 25,162,254 | 78,496,604 | 0.0272 | 0.1453 | 0.0305 | 0.0715 |
| 203 | 25,070,487 | 78,599,703 | 0.0272 | 0.1449 | 0.0301 | 0.0713 |
| 204 | 24,976,370 | 78,699,354 | 0.0264 | 0.1455 | 0.0297 | 0.0719 |
| 205 | 24,879,582 | 78,795,763 | 0.0269 | 0.1465 | 0.0301 | 0.0698 |
| 206 | 24,780,091 | 78,888,736 | 0.0280 | 0.1502 | 0.0291 | 0.0700 |
| 207 | 24,677,742 | 78,978,233 | 0.0272 | 0.1482 | 0.0287 | 0.0695 |
| 208 | 24,572,261 | 79,064,276 | 0.0271 | 0.1474 | 0.0287 | 0.0707 |
| 209 | 24,463,663 | 79,146,714 | 0.0258 | 0.1456 | 0.0295 | 0.0702 |
| 210 | 24,351,844 | 79,225,481 | 0.0256 | 0.1472 | 0.0298 | 0.0710 |
| 211 | 24,236,872 | 79,300,584 | 0.0256 | 0.1508 | 0.0248 | 0.0719 |
| 212 | 24,118,998 | 79,372,082 | 0.0261 | 0.1462 | 0.0287 | 0.0704 |
| 213 | 23,998,051 | 79,440,053 | 0.0250 | 0.1462 | 0.0287 | 0.0700 |
| 214 | 23,874,380 | 79,504,504 | 0.0252 | 0.1481 | 0.0284 | 0.0700 |
| 215 | 23,748,149 | 79,565,625 | 0.0251 | 0.1433 | 0.0286 | 0.0704 |
| 216 | 23,619,917 | 79,623,487 | 0.0243 | 0.1493 | 0.0281 | 0.0705 |
| 217 | 23,489,657 | 79,678,253 | 0.0271 | 0.1446 | 0.0282 | 0.0709 |
| 218 | 23,357,630 | 79,730,071 | 0.0258 | 0.1459 | 0.0287 | 0.0688 |
| 219 | 23,223,665 | 79,779,016 | 0.0247 | 0.1460 | 0.0282 | 0.0699 |
| 220 | 23,087,883 | 79,825,181 | 0.0242 | 0.1461 | 0.0276 | 0.0688 |
| 221 | 22,950,355 | 79,868,670 | 0.0243 | 0.1452 | 0.0272 | 0.0686 |
| 222 | 22,810,876 | 79,909,515 | 0.0242 | 0.1449 | 0.0271 | 0.0687 |
| 223 | 22,670,014 | 79,947,770 | 0.0241 | 0.1445 | 0.0274 | 0.0694 |
| 224 | 22,527,520 | 79,983,633 | 0.0236 | 0.1458 | 0.0265 | 0.0680 |
| 225 | 22,383,535 | 80,017,213 | 0.0236 | 0.1449 | 0.0264 | 0.0694 |
| 226 | 22,238,007 | 80,048,760 | 0.0231 | 0.1444 | 0.0272 | 0.0702 |
| 227 | 22,090,965 | 80,078,463 | 0.0234 | 0.1443 | 0.0264 | 0.0688 |
| 228 | 21,942,371 | 80,106,504 | 0.0235 | 0.1441 | 0.0260 | 0.0689 |
| 229 | 21,792,483 | 80,133,134 | 0.0244 | 0.1461 | 0.0261 | 0.0686 |
| 230 | 21,641,274 | 80,158,393 | 0.0233 | 0.1436 | 0.0256 | 0.0672 |
| 231 | 21,488,719 | 80,182,374 | 0.0226 | 0.1441 | 0.0257 | 0.0678 |
| 232 | 21,335,006 | 80,205,090 | 0.0260 | 0.1461 | 0.0250 | 0.0691 |
| 233 | 21,180,264 | 80,226,530 | 0.0228 | 0.1433 | 0.0250 | 0.0678 |
| 234 | 21,024,345 | 80,246,765 | 0.0229 | 0.1435 | 0.0249 | 0.0687 |
| 235 | 20,867,254 | 80,265,670 | 0.0229 | 0.1447 | 0.0236 | 0.0702 |
| 236 | 20,709,354 | 80,283,284 | 0.0225 | 0.1435 | 0.0246 | 0.0680 |
| 237 | 20,551,039 | 80,299,769 | 0.0216 | 0.1445 | 0.0252 | 0.0681 |
| 238 | 20,392,609 | 80,315,319 | 0.0223 | 0.1433 | 0.0246 | 0.0683 |
| 239 | 20,234,025 | 80,330,019 | 0.0218 | 0.1445 | 0.0238 | 0.0681 |
| 240 | 20,075,387 | 80,343,919 | 0.0215 | 0.1446 | 0.0231 | 0.0689 |
| 241 | 19,916,531 | 80,357,024 | 0.0229 | 0.1437 | 0.0235 | 0.0675 |
| 242 | 19,757,363 | 80,369,267 | 0.0218 | 0.1493 | 0.0239 | 0.0663 |
| 243 | 19,597,812 | 80,380,641 | 0.0217 | 0.1419 | 0.0230 | 0.0682 |
| 244 | 19,438,135 | 80,391,114 | 0.0219 | 0.1425 | 0.0238 | 0.0686 |
| 245 | 19,278,237 | 80,400,766 | 0.0232 | 0.1432 | 0.0225 | 0.0683 |
| 246 | 19,118,218 | 80,409,684 | 0.0220 | 0.2057 | 0.0186 | 0.0685 |
| 247 | 18,958,400 | 80,417,910 | 0.0207 | 0.1421 | 0.0222 | 0.0672 |
| 248 | 18,798,681 | 80,425,562 | 0.0204 | 0.1411 | 0.0219 | 0.0678 |
| 249 | 18,639,111 | 80,432,654 | 0.0209 | 0.1406 | 0.0222 | 0.0654 |
| 250 | 18,479,683 | 80,439,173 | 0.0202 | 0.1425 | 0.0216 | 0.0666 |
| 251 | 18,320,270 | 80,445,155 | 0.0203 | 0.1406 | 0.0214 | 0.0667 |
| 252 | 18,160,756 | 80,450,578 | 0.0202 | 0.1420 | 0.0221 | 0.0668 |
| 253 | 18,001,345 | 80,455,469 | 0.0197 | 0.1423 | 0.0216 | 0.0666 |
| 254 | 17,841,981 | 80,459,897 | 0.0201 | 0.1449 | 0.0175 | 0.0659 |
| 255 | 17,682,900 | 80,463,897 | 0.0243 | 0.1413 | 0.0206 | 0.0653 |
| 256 | 17,524,093 | 80,467,539 | 0.0197 | 0.1424 | 0.0207 | 0.0664 |
| 257 | 17,365,486 | 80,470,869 | 0.0187 | 0.1408 | 0.0209 | 0.0662 |
| 258 | 17,207,108 | 80,473,927 | 0.0189 | 0.1408 | 0.0203 | 0.0655 |
| 259 | 17,049,076 | 80,476,772 | 0.0191 | 0.1400 | 0.0203 | 0.0665 |
| 260 | 16,891,512 | 80,479,391 | 0.0190 | 0.1411 | 0.0204 | 0.0653 |
| 261 | 16,734,574 | 80,481,772 | 0.0183 | 0.1405 | 0.0194 | 0.0669 |
| 262 | 16,578,492 | 80,483,924 | 0.0186 | 0.1404 | 0.0199 | 0.0653 |
| 263 | 16,423,170 | 80,485,843 | 0.0195 | 0.1408 | 0.0191 | 0.0660 |
| 264 | 16,268,502 | 80,487,531 | 0.0183 | 0.1414 | 0.0186 | 0.0650 |
| 265 | 16,114,173 | 80,489,028 | 0.0179 | 0.1409 | 0.0192 | 0.0643 |
| 266 | 15,960,159 | 80,490,346 | 0.0188 | 0.1391 | 0.0192 | 0.0660 |
| 267 | 15,806,341 | 80,491,511 | 0.0188 | 0.1450 | 0.0194 | 0.0651 |
| 268 | 15,652,613 | 80,492,545 | 0.0190 | 0.1417 | 0.0187 | 0.0648 |
| 269 | 15,498,985 | 80,493,456 | 0.0177 | 0.1946 | 0.0187 | 0.0652 |
| 270 | 15,345,338 | 80,494,260 | 0.0189 | 0.1394 | 0.0533 | 0.1003 |
| 271 | 15,191,697 | 80,494,968 | 0.0175 | 0.1392 | 0.0184 | 0.0663 |
| 272 | 15,037,983 | 80,495,572 | 0.0170 | 0.1387 | 0.0179 | 0.0655 |
| 273 | 14,884,368 | 80,496,081 | 0.0179 | 0.1392 | 0.0180 | 0.0638 |
| 274 | 14,731,165 | 80,496,493 | 0.0181 | 0.1387 | 0.0172 | 0.0645 |
| 275 | 14,578,412 | 80,496,825 | 0.0178 | 0.1394 | 0.0177 | 0.0645 |
| 276 | 14,426,338 | 80,497,095 | 0.0164 | 0.1395 | 0.0171 | 0.0647 |
| 277 | 14,274,975 | 80,497,309 | 0.0168 | 0.1363 | 0.0174 | 0.0631 |
| 278 | 14,124,386 | 80,497,473 | 0.0172 | 0.1399 | 0.0158 | 0.0650 |
| 279 | 13,974,676 | 80,497,614 | 0.0161 | 0.1385 | 0.0165 | 0.0654 |
| 280 | 13,826,024 | 80,497,730 | 0.0157 | 0.1380 | 0.0170 | 0.0633 |
| 281 | 13,678,425 | 80,497,823 | 0.0169 | 0.1387 | 0.0160 | 0.0642 |
| 282 | 13,532,088 | 80,497,895 | 0.0158 | 0.1372 | 0.0168 | 0.0635 |
| 283 | 13,386,931 | 80,497,948 | 0.0168 | 0.1370 | 0.0155 | 0.0644 |
| 284 | 13,242,862 | 80,497,984 | 0.0155 | 0.1375 | 0.0161 | 0.0631 |
| 285 | 13,099,988 | 80,498,003 | 0.0154 | 0.2923 | 0.0156 | 0.0642 |
| 286 | 12,958,176 | 80,498,011 | 0.0173 | 0.2802 | 0.0150 | 0.0633 |
| 287 | 12,817,350 | 80,498,014 | 0.0190 | 0.1398 | 0.0148 | 0.0634 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| SF.cedge | 223,001 | 80,498,014 | 287 | 3,456 x 1,024 | 70.1464 |


Initialization: 1.3879, Read: 0.0424, reverse: 0.0008
Hashtable rate: 6,406,970,062 keys/s, time: 0.0000
Join: 5.6243
Projection: 5.9024
Deduplication: 30.2819
Memory clear: 12.7535
Union: 14.1532
Total: 70.1464

Benchmark for p2p-Gnutella09
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 105,493 | 26,013 | 0.0017 | 0.0032 | 0.0002 | 0.0004 |
| 2 | 404,904 | 129,548 | 0.0018 | 0.0041 | 0.0005 | 0.0006 |
| 3 | 1,325,000 | 505,177 | 0.0025 | 0.0057 | 0.0013 | 0.0013 |
| 4 | 3,600,153 | 1,621,937 | 0.0052 | 0.0109 | 0.0036 | 0.0038 |
| 5 | 7,770,985 | 4,262,546 | 0.0094 | 0.0202 | 0.0089 | 0.0076 |
| 6 | 12,779,550 | 8,761,434 | 0.0183 | 0.0337 | 0.0176 | 0.0145 |
| 7 | 16,677,166 | 13,700,045 | 0.0277 | 0.0476 | 0.0250 | 0.0213 |
| 8 | 18,909,848 | 17,258,403 | 0.0366 | 0.0564 | 0.0289 | 0.0255 |
| 9 | 20,042,582 | 19,214,882 | 0.0403 | 0.0614 | 0.0314 | 0.0259 |
| 10 | 20,657,402 | 20,204,059 | 0.0435 | 0.0634 | 0.0330 | 0.0291 |
| 11 | 21,009,215 | 20,751,684 | 0.0430 | 0.0649 | 0.0337 | 0.0294 |
| 12 | 21,211,485 | 21,064,266 | 0.0433 | 0.0664 | 0.0339 | 0.0298 |
| 13 | 21,322,255 | 21,243,127 | 0.0448 | 0.2135 | 0.0327 | 0.0321 |
| 14 | 21,375,212 | 21,339,158 | 0.0451 | 0.0662 | 0.0345 | 0.0293 |
| 15 | 21,394,456 | 21,382,486 | 0.0442 | 0.2131 | 0.0329 | 0.0331 |
| 16 | 21,400,317 | 21,397,410 | 0.0468 | 0.3048 | 0.0331 | 0.0336 |
| 17 | 21,401,624 | 21,401,928 | 0.0565 | 0.2850 | 0.0338 | 0.0387 |
| 18 | 21,401,798 | 21,402,851 | 0.0594 | 0.2779 | 0.0334 | 0.0442 |
| 19 | 21,401,809 | 21,402,957 | 0.0581 | 0.2741 | 0.0332 | 0.0465 |
| 20 | 21,401,809 | 21,402,960 | 0.0574 | 0.2731 | 0.0336 | 0.0481 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| p2p-Gnutella09 | 26,013 | 21,402,960 | 20 | 3,456 x 1,024 | 4.7777 |


Initialization: 0.0005, Read: 0.0113, reverse: 0.0003
Hashtable rate: 462,733,029 keys/s, time: 0.0001
Join: 0.6856
Projection: 0.4852
Deduplication: 2.3455
Memory clear: 0.7543
Union: 0.4948
Total: 4.7777

Benchmark for p2p-Gnutella04
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 179,268 | 39,994 | 0.0020 | 0.0035 | 0.0002 | 0.0004 |
| 2 | 774,471 | 218,370 | 0.0019 | 0.0093 | 0.0011 | 0.0009 |
| 3 | 3,098,417 | 976,616 | 0.0036 | 0.0093 | 0.0033 | 0.0030 |
| 4 | 9,923,248 | 3,842,060 | 0.0090 | 0.0221 | 0.0112 | 0.0096 |
| 5 | 21,660,016 | 11,653,146 | 0.0258 | 0.0516 | 0.0292 | 0.0230 |
| 6 | 32,626,065 | 23,748,922 | 0.0553 | 0.3567 | 0.0561 | 0.1134 |
| 7 | 39,357,207 | 34,088,772 | 0.1066 | 0.4984 | 0.0348 | 0.1706 |
| 8 | 42,840,923 | 40,152,911 | 0.1278 | 0.5193 | 0.0699 | 0.1531 |
| 9 | 44,597,885 | 43,246,096 | 0.1303 | 0.5769 | 0.0690 | 0.1332 |
| 10 | 45,562,373 | 44,812,737 | 0.1308 | 0.5953 | 0.0751 | 0.1265 |
| 11 | 46,149,143 | 45,689,891 | 0.1368 | 0.6055 | 0.0759 | 0.1216 |
| 12 | 46,495,830 | 46,226,844 | 0.1380 | 0.6126 | 0.0776 | 0.1193 |
| 13 | 46,686,382 | 46,539,051 | 0.1360 | 0.6274 | 0.0773 | 0.1167 |
| 14 | 46,798,445 | 46,710,511 | 0.1371 | 0.6276 | 0.0779 | 0.1118 |
| 15 | 46,872,117 | 46,813,927 | 0.1377 | 0.6276 | 0.0790 | 0.1141 |
| 16 | 46,927,744 | 46,883,084 | 0.1381 | 0.6262 | 0.0780 | 0.1171 |
| 17 | 46,976,810 | 46,937,127 | 0.1382 | 0.6283 | 0.0777 | 0.1125 |
| 18 | 47,017,268 | 46,984,995 | 0.1376 | 0.6294 | 0.0780 | 0.1154 |
| 19 | 47,042,974 | 47,023,338 | 0.1391 | 0.6272 | 0.0784 | 0.1130 |
| 20 | 47,054,749 | 47,046,747 | 0.1393 | 0.6305 | 0.0782 | 0.1123 |
| 21 | 47,057,636 | 47,056,798 | 0.1417 | 0.6315 | 0.0790 | 0.1144 |
| 22 | 47,058,085 | 47,059,086 | 0.1409 | 0.6330 | 0.0780 | 0.1118 |
| 23 | 47,058,153 | 47,059,447 | 0.1389 | 0.6333 | 0.0782 | 0.1147 |
| 24 | 47,058,172 | 47,059,508 | 0.1439 | 0.6363 | 0.0789 | 0.1124 |
| 25 | 47,058,176 | 47,059,523 | 0.1402 | 0.6355 | 0.0782 | 0.1124 |
| 26 | 47,058,176 | 47,059,527 | 0.1398 | 0.6355 | 0.0779 | 0.1140 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| p2p-Gnutella04 | 39,994 | 47,059,527 | 26 | 3,456 x 1,024 | 22.1751 |


Initialization: 0.0008, Read: 0.0079, reverse: 0.0006
Hashtable rate: 527,806,371 keys/s, time: 0.0001
Join: 2.8165
Projection: 1.5980
Deduplication: 12.6899
Memory clear: 2.4938
Union: 2.5674
Total: 22.1751

Benchmark for cal.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 19,835 | 21,693 | 0.0019 | 0.0034 | 0.0001 | 0.0002 |
| 2 | 18,616 | 41,526 | 0.0015 | 0.0032 | 0.0000 | 0.0001 |
| 3 | 17,604 | 60,132 | 0.0015 | 0.0031 | 0.0000 | 0.0002 |
| 4 | 16,703 | 77,715 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 5 | 15,876 | 94,383 | 0.0015 | 0.0032 | 0.0000 | 0.0002 |
| 6 | 15,118 | 110,206 | 0.0015 | 0.0034 | 0.0000 | 0.0000 |
| 7 | 14,398 | 125,261 | 0.0015 | 0.0037 | 0.0000 | 0.0000 |
| 8 | 13,712 | 139,584 | 0.0015 | 0.0034 | 0.0000 | 0.0000 |
| 9 | 13,048 | 153,218 | 0.0015 | 0.0036 | 0.0000 | 0.0000 |
| 10 | 12,415 | 166,192 | 0.0015 | 0.0036 | 0.0000 | 0.0000 |
| 11 | 11,825 | 178,538 | 0.0015 | 0.0034 | 0.0000 | 0.0006 |
| 12 | 11,283 | 190,304 | 0.0015 | 0.0035 | 0.0000 | 0.0003 |
| 13 | 10,773 | 201,538 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 14 | 10,305 | 212,277 | 0.0015 | 0.0033 | 0.0000 | 0.0003 |
| 15 | 9,852 | 222,554 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 16 | 9,431 | 232,384 | 0.0034 | 0.0035 | 0.0000 | 0.0003 |
| 17 | 9,032 | 241,797 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 18 | 8,657 | 250,810 | 0.0015 | 0.0033 | 0.0000 | 0.0003 |
| 19 | 8,313 | 259,449 | 0.0015 | 0.0035 | 0.0000 | 0.0004 |
| 20 | 7,985 | 267,745 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 21 | 7,668 | 275,713 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 22 | 7,361 | 283,363 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 23 | 7,079 | 290,703 | 0.0015 | 0.0036 | 0.0000 | 0.0003 |
| 24 | 6,818 | 297,762 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 25 | 6,568 | 304,561 | 0.0015 | 0.0037 | 0.0000 | 0.0004 |
| 26 | 6,335 | 311,114 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 27 | 6,113 | 317,434 | 0.0015 | 0.0037 | 0.0000 | 0.0004 |
| 28 | 5,891 | 323,532 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 29 | 5,680 | 329,409 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 30 | 5,477 | 335,075 | 0.0015 | 0.0040 | 0.0000 | 0.0005 |
| 31 | 5,283 | 340,538 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 32 | 5,102 | 345,808 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 33 | 4,933 | 350,899 | 0.0015 | 0.0037 | 0.0000 | 0.0008 |
| 34 | 4,762 | 355,820 | 0.0015 | 0.0145 | 0.0000 | 0.0005 |
| 35 | 4,595 | 360,571 | 0.0116 | 0.0187 | 0.0000 | 0.0005 |
| 36 | 4,438 | 365,155 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 37 | 4,283 | 369,583 | 0.0015 | 0.0037 | 0.0000 | 0.0007 |
| 38 | 4,134 | 373,855 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 39 | 3,989 | 377,981 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 40 | 3,856 | 381,964 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 41 | 3,723 | 385,813 | 0.0016 | 0.0037 | 0.0000 | 0.0005 |
| 42 | 3,596 | 389,531 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 43 | 3,477 | 393,123 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 44 | 3,355 | 396,594 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 45 | 3,226 | 399,943 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 46 | 3,106 | 403,163 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 47 | 2,987 | 406,263 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 48 | 2,879 | 409,243 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 49 | 2,782 | 412,116 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 50 | 2,691 | 414,893 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 51 | 2,601 | 417,579 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 52 | 2,515 | 420,175 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 53 | 2,432 | 422,685 | 0.0032 | 0.0038 | 0.0000 | 0.0009 |
| 54 | 2,346 | 425,112 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 55 | 2,264 | 427,454 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 56 | 2,190 | 429,715 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 57 | 2,120 | 431,902 | 0.0015 | 0.0040 | 0.0000 | 0.0008 |
| 58 | 2,052 | 434,019 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 59 | 1,984 | 436,068 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 60 | 1,920 | 438,049 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 61 | 1,855 | 439,966 | 0.0014 | 0.0041 | 0.0000 | 0.0006 |
| 62 | 1,797 | 441,818 | 0.0015 | 0.0041 | 0.0000 | 0.0006 |
| 63 | 1,739 | 443,611 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 64 | 1,682 | 445,347 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 65 | 1,629 | 447,026 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 66 | 1,580 | 448,652 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 67 | 1,531 | 450,229 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 68 | 1,484 | 451,757 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 69 | 1,436 | 453,238 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 70 | 1,391 | 454,671 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 71 | 1,347 | 456,059 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 72 | 1,306 | 457,403 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 73 | 1,272 | 458,706 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 74 | 1,237 | 459,973 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 75 | 1,200 | 461,205 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 76 | 1,167 | 462,400 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 77 | 1,134 | 463,562 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 78 | 1,101 | 464,691 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 79 | 1,069 | 465,787 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 80 | 1,034 | 466,851 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 81 | 1,002 | 467,882 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 82 | 972 | 468,881 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 83 | 940 | 469,850 | 0.0016 | 0.0039 | 0.0000 | 0.0006 |
| 84 | 912 | 470,787 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 85 | 885 | 471,696 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 86 | 861 | 472,578 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 87 | 837 | 473,436 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 88 | 813 | 474,270 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 89 | 790 | 475,080 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 90 | 768 | 475,867 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 91 | 746 | 476,632 | 0.0014 | 0.0038 | 0.0000 | 0.0006 |
| 92 | 725 | 477,375 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 93 | 706 | 478,097 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 94 | 685 | 478,800 | 0.0024 | 0.0039 | 0.0000 | 0.0006 |
| 95 | 664 | 479,482 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 96 | 646 | 480,145 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 97 | 627 | 480,790 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 98 | 610 | 481,416 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 99 | 591 | 482,025 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 100 | 575 | 482,615 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 101 | 559 | 483,189 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 102 | 545 | 483,747 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 103 | 531 | 484,291 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 104 | 516 | 484,821 | 0.0015 | 0.0038 | 0.0000 | 0.0009 |
| 105 | 501 | 485,336 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 106 | 486 | 485,836 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 107 | 472 | 486,321 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 108 | 458 | 486,792 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 109 | 443 | 487,249 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 110 | 429 | 487,691 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 111 | 415 | 488,119 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 112 | 402 | 488,533 | 0.0014 | 0.0039 | 0.0000 | 0.0008 |
| 113 | 391 | 488,934 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 114 | 379 | 489,324 | 0.0016 | 0.0039 | 0.0000 | 0.0009 |
| 115 | 367 | 489,701 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 116 | 357 | 490,067 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 117 | 347 | 490,423 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 118 | 338 | 490,769 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 119 | 330 | 491,106 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 120 | 321 | 491,435 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 121 | 313 | 491,755 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 122 | 307 | 492,067 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 123 | 303 | 492,373 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 124 | 297 | 492,675 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 125 | 291 | 492,971 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 126 | 287 | 493,261 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 127 | 282 | 493,547 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 128 | 279 | 493,828 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 129 | 276 | 494,106 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 130 | 272 | 494,381 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 131 | 268 | 494,652 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 132 | 263 | 494,919 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 133 | 259 | 495,181 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 134 | 252 | 495,439 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 135 | 245 | 495,690 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 136 | 237 | 495,934 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 137 | 230 | 496,170 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 138 | 223 | 496,399 | 0.0017 | 0.0039 | 0.0000 | 0.0008 |
| 139 | 215 | 496,621 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 140 | 206 | 496,835 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 141 | 201 | 497,040 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 142 | 195 | 497,240 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 143 | 190 | 497,434 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 144 | 183 | 497,623 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 145 | 176 | 497,805 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 146 | 170 | 497,980 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 147 | 164 | 498,149 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 148 | 158 | 498,312 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 149 | 152 | 498,469 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 150 | 146 | 498,620 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 151 | 139 | 498,765 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 152 | 133 | 498,903 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 153 | 128 | 499,035 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 154 | 123 | 499,162 | 0.0015 | 0.0042 | 0.0000 | 0.0005 |
| 155 | 117 | 499,284 | 0.0016 | 0.0038 | 0.0000 | 0.0007 |
| 156 | 114 | 499,400 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 157 | 112 | 499,513 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 158 | 110 | 499,624 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 159 | 108 | 499,733 | 0.0015 | 0.0038 | 0.0000 | 0.0009 |
| 160 | 106 | 499,840 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 161 | 104 | 499,945 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 162 | 102 | 500,048 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 163 | 100 | 500,149 | 0.0014 | 0.0038 | 0.0000 | 0.0008 |
| 164 | 98 | 500,248 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 165 | 96 | 500,345 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 166 | 94 | 500,440 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 167 | 92 | 500,533 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 168 | 90 | 500,624 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 169 | 88 | 500,713 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 170 | 85 | 500,799 | 0.0015 | 0.0041 | 0.0000 | 0.0006 |
| 171 | 82 | 500,882 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 172 | 79 | 500,962 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 173 | 76 | 501,039 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 174 | 73 | 501,113 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 175 | 69 | 501,184 | 0.0015 | 0.0061 | 0.0000 | 0.0006 |
| 176 | 64 | 501,251 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 177 | 60 | 501,314 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 178 | 55 | 501,373 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 179 | 50 | 501,427 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 180 | 45 | 501,476 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 181 | 40 | 501,520 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 182 | 35 | 501,559 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 183 | 31 | 501,593 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 184 | 27 | 501,623 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 185 | 24 | 501,649 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 186 | 21 | 501,672 | 0.0015 | 0.0042 | 0.0000 | 0.0006 |
| 187 | 18 | 501,692 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 188 | 15 | 501,709 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 189 | 12 | 501,723 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 190 | 9 | 501,734 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 191 | 6 | 501,742 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 192 | 4 | 501,748 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 193 | 2 | 501,752 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 194 | 1 | 501,754 | 0.0015 | 0.0038 | 0.0000 | 0.0009 |
| 195 | 0 | 501,755 | 0.0015 | 0.0032 | 0.0000 | 0.0006 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| cal.cedge | 21,693 | 501,755 | 195 | 3,456 x 1,024 | 1.2287 |


Initialization: 0.0005, Read: 0.0045, reverse: 0.0003
Hashtable rate: 1,143,783,612 keys/s, time: 0.0000
Join: 0.3020
Projection: 0.0037
Deduplication: 0.7681
Memory clear: 0.0351
Union: 0.1145
Total: 1.2287

Benchmark for TG.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 22,471 | 23,874 | 0.0015 | 0.0032 | 0.0000 | 0.0000 |
| 2 | 22,375 | 45,838 | 0.0015 | 0.0032 | 0.0000 | 0.0001 |
| 3 | 23,637 | 66,624 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 4 | 25,782 | 87,014 | 0.0015 | 0.0033 | 0.0000 | 0.0003 |
| 5 | 28,560 | 107,321 | 0.0014 | 0.0031 | 0.0000 | 0.0000 |
| 6 | 31,840 | 127,667 | 0.0015 | 0.0035 | 0.0000 | 0.0000 |
| 7 | 35,460 | 148,007 | 0.0015 | 0.0035 | 0.0000 | 0.0000 |
| 8 | 39,337 | 168,222 | 0.0017 | 0.0042 | 0.0000 | 0.0004 |
| 9 | 43,371 | 188,149 | 0.0125 | 0.0079 | 0.0000 | 0.0004 |
| 10 | 47,414 | 207,747 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 11 | 51,351 | 226,937 | 0.0015 | 0.0035 | 0.0000 | 0.0004 |
| 12 | 55,127 | 245,569 | 0.0015 | 0.0036 | 0.0000 | 0.0005 |
| 13 | 58,670 | 263,578 | 0.0015 | 0.0037 | 0.0000 | 0.0009 |
| 14 | 62,029 | 280,898 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 15 | 65,016 | 297,495 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 16 | 67,792 | 313,305 | 0.0015 | 0.0038 | 0.0001 | 0.0005 |
| 17 | 70,145 | 328,407 | 0.0015 | 0.0039 | 0.0001 | 0.0005 |
| 18 | 71,909 | 342,636 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 19 | 73,064 | 355,917 | 0.0015 | 0.0038 | 0.0001 | 0.0005 |
| 20 | 73,683 | 368,221 | 0.0015 | 0.0057 | 0.0001 | 0.0004 |
| 21 | 73,821 | 379,601 | 0.0015 | 0.0039 | 0.0000 | 0.0005 |
| 22 | 73,437 | 390,144 | 0.0016 | 0.0040 | 0.0001 | 0.0006 |
| 23 | 72,723 | 399,786 | 0.0015 | 0.0040 | 0.0001 | 0.0005 |
| 24 | 71,591 | 408,600 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 25 | 70,144 | 416,622 | 0.0017 | 0.0040 | 0.0001 | 0.0005 |
| 26 | 68,467 | 423,911 | 0.0015 | 0.0039 | 0.0001 | 0.0005 |
| 27 | 66,570 | 430,545 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 28 | 64,480 | 436,571 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 29 | 62,259 | 442,041 | 0.0015 | 0.0039 | 0.0002 | 0.0006 |
| 30 | 59,796 | 446,999 | 0.0015 | 0.0039 | 0.0001 | 0.0006 |
| 31 | 57,243 | 451,424 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 32 | 54,559 | 455,377 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 33 | 51,796 | 458,896 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 34 | 49,012 | 461,978 | 0.0015 | 0.0041 | 0.0000 | 0.0006 |
| 35 | 46,217 | 464,659 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 36 | 43,494 | 467,015 | 0.0015 | 0.0040 | 0.0000 | 0.0007 |
| 37 | 40,877 | 469,123 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 38 | 38,399 | 471,008 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 39 | 35,988 | 472,687 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 40 | 33,679 | 474,152 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 41 | 31,442 | 475,408 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 42 | 29,250 | 476,476 | 0.0015 | 0.0040 | 0.0001 | 0.0005 |
| 43 | 27,107 | 477,382 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 44 | 25,048 | 478,152 | 0.0015 | 0.0040 | 0.0000 | 0.0007 |
| 45 | 23,074 | 478,795 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 46 | 21,168 | 479,331 | 0.0016 | 0.0039 | 0.0000 | 0.0007 |
| 47 | 19,362 | 479,761 | 0.0015 | 0.0040 | 0.0000 | 0.0009 |
| 48 | 17,640 | 480,103 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 49 | 16,033 | 480,376 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 50 | 14,527 | 480,597 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 51 | 13,113 | 480,774 | 0.0015 | 0.0039 | 0.0000 | 0.0005 |
| 52 | 11,765 | 480,906 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 53 | 10,504 | 480,998 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 54 | 9,324 | 481,059 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 55 | 8,247 | 481,096 | 0.0015 | 0.0042 | 0.0000 | 0.0009 |
| 56 | 7,270 | 481,114 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 57 | 6,366 | 481,120 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 58 | 5,573 | 481,121 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| TG.cedge | 23,874 | 481,121 | 58 | 3,456 x 1,024 | 0.3733 |


Initialization: 0.0003, Read: 0.0048, reverse: 0.0002
Hashtable rate: 1,193,222,710 keys/s, time: 0.0000
Join: 0.0972
Projection: 0.0022
Deduplication: 0.2283
Memory clear: 0.0098
Union: 0.0305
Total: 0.3733

Benchmark for OL.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 7,331 | 7,035 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 2 | 7,628 | 14,319 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 3 | 7,848 | 21,813 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 4 | 8,017 | 29,352 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 5 | 8,134 | 36,851 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 6 | 8,214 | 44,235 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 7 | 8,263 | 51,441 | 0.0015 | 0.0030 | 0.0000 | 0.0002 |
| 8 | 8,293 | 58,424 | 0.0015 | 0.0030 | 0.0000 | 0.0004 |
| 9 | 8,321 | 65,147 | 0.0015 | 0.0031 | 0.0000 | 0.0003 |
| 10 | 8,317 | 71,596 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 11 | 8,357 | 77,694 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 12 | 8,415 | 83,457 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 13 | 8,448 | 88,894 | 0.0015 | 0.0032 | 0.0000 | 0.0003 |
| 14 | 8,467 | 93,984 | 0.0015 | 0.0034 | 0.0000 | 0.0000 |
| 15 | 8,463 | 98,738 | 0.0015 | 0.0032 | 0.0000 | 0.0005 |
| 16 | 8,424 | 103,161 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 17 | 8,352 | 107,252 | 0.0015 | 0.0032 | 0.0000 | 0.0005 |
| 18 | 8,248 | 111,015 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 19 | 8,133 | 114,460 | 0.0015 | 0.0032 | 0.0000 | 0.0005 |
| 20 | 7,979 | 117,619 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 21 | 7,754 | 120,475 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 22 | 7,541 | 123,011 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 23 | 7,341 | 125,308 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 24 | 7,127 | 127,436 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 25 | 6,877 | 129,399 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 26 | 6,591 | 131,199 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 27 | 6,259 | 132,828 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 28 | 5,883 | 134,275 | 0.0014 | 0.0031 | 0.0000 | 0.0000 |
| 29 | 5,469 | 135,536 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 30 | 5,038 | 136,633 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 31 | 4,611 | 137,589 | 0.0014 | 0.0031 | 0.0000 | 0.0000 |
| 32 | 4,192 | 138,429 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 33 | 3,797 | 139,178 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 34 | 3,427 | 139,847 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 35 | 3,071 | 140,452 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 36 | 2,726 | 140,995 | 0.0017 | 0.0031 | 0.0000 | 0.0000 |
| 37 | 2,414 | 141,481 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 38 | 2,134 | 141,924 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 39 | 1,887 | 142,327 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 40 | 1,665 | 142,694 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 41 | 1,467 | 143,034 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 42 | 1,280 | 143,360 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 43 | 1,109 | 143,675 | 0.0016 | 0.0030 | 0.0000 | 0.0000 |
| 44 | 960 | 143,976 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 45 | 824 | 144,264 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 46 | 709 | 144,532 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 47 | 608 | 144,779 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 48 | 523 | 145,002 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 49 | 445 | 145,198 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 50 | 379 | 145,369 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 51 | 322 | 145,518 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 52 | 272 | 145,645 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 53 | 227 | 145,755 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 54 | 190 | 145,847 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 55 | 158 | 145,922 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 56 | 130 | 145,980 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 57 | 105 | 146,024 | 0.0015 | 0.0032 | 0.0000 | 0.0000 |
| 58 | 81 | 146,057 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 59 | 61 | 146,082 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 60 | 44 | 146,098 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 61 | 34 | 146,109 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 62 | 24 | 146,116 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 63 | 17 | 146,119 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 64 | 12 | 146,120 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| OL.cedge | 7,035 | 146,120 | 64 | 3,456 x 1,024 | 0.3000 |


Initialization: 0.0003, Read: 0.0016, reverse: 0.0001
Hashtable rate: 353,215,845 keys/s, time: 0.0000
Join: 0.0941
Projection: 0.0012
Deduplication: 0.1962
Memory clear: 0.0020
Union: 0.0045
Total: 0.3000



## Day 2 Progress
- Generated datasets:
  - 100000 to 1000000, interval 50000

## Comparison between Rapids cudf, pandas df, and nested loop join

### Rapids
- Implemented two versions using `rapids` `cudf` and `pandas` `df`.

| Number of rows | CUDF time (s) | Pandas time (s) |
| --- | --- | --- |
| 100000 | 0.052770 | 0.282879 |
| 150000 | 0.105069 | 0.912774 |

Our nested loop join performance:

| Number of rows | #Blocks | #Threads | #Result rows | Pass 1 | Offset calculation | Pass 2 | Total time |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 100000 | 196 | 512 | 20000986 | 0.0433023 | 0.000323834 | 0.15379 | 0.197416 |
| 100000 | 98 | 1024 | 20000986 | 0.0473048 | 0.000349723 | 0.169904 | 0.217558 |
| 150000 | 293 | 512 | 44995231 | 0.0917667 | 0.000335558 | 0.34609 | 0.438192 |
| 150000 | 147 | 1024 | 44995231 | 0.115846 | 0.00314425 | 0.378974 | 0.497964 |


For cudf we are getting error for `n=200000`:
```
std::bad_alloc: out_of_memory: CUDA error at: /workspace/.conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory
```
- Tried to create conda environment in Theta GPU. Got error:
```shell
(miniconda-3/latest/base) arsho@thetalogin4:~> conda create -p /envs/gpujoin_env --clone $CONDA_PREFIX
Source:      /soft/datascience/conda/miniconda3/latest
Destination: /envs/gpujoin_env
The following packages cannot be cloned out of the root environment:
 - defaults/linux-64::conda-4.8.3-py37_0
Packages: 175
Files: 117276
```

### Cuda
#### Nested loop join dynamic size
- Removed `cudaMemcpy` for host to device and device to host transfer and vice versa
- Added `cudaMallocManaged`
- Removed CPU offset calculation with Thrust's exclusive scan
- Added benchmark
- Gathered report from `nsys` and `nvprof`: 
```shell
nvprof ./join                                
GPU join operation (non-atomic): (500000, 2) x (500000, 2)
Blocks per grid: 489, Threads per block: 1024
==69548== NVPROF is profiling process 69548, command: ./join
GPU Pass 1 get join size per row in relation 1: 1.03881 seconds
Total size of the join result: 1499895627
Thrust calculate offset: 0.000433121 seconds
GPU Pass 2 join operation: 4.41225 seconds
Total time (pass 1 + offset + pass 2): 5.45149
| Number of rows | #Blocks | #Threads | #Result rows | Pass 1 | Offset calculation | Pass 2 | Total time |
| 500000 | 489 | 1024 | 499965209 | 1.03881 | 0.000433121 | 4.41225 | 5.45149 |

==69548== Profiling application: ./join
==69548== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   80.94%  4.41223s         1  4.41223s  4.41223s  4.41223s  gpu_get_join_data_dynamic(int*, int*, int*, int, int, int, int*, int, int, int)
                   19.06%  1.03876s         1  1.03876s  1.03876s  1.03876s  gpu_get_join_size_per_thread(int*, int*, int, int, int, int*, int, int, int)
                    0.00%  29.792us         1  29.792us  29.792us  29.792us  void cub::DeviceScanKernel<cub::DeviceScanPolicy<int>::Policy600, int*, int*, cub::ScanTileState<int, bool=1>, thrust::plus<void>, cub::detail::InputValue<int, int*>, int>(cub::DeviceScanPolicy<int>::Policy600, int*, int*, int, int, bool=1, cub::ScanTileState<int, bool=1>)
                    0.00%  16.831us         1  16.831us  16.831us  16.831us  void cub::DeviceReduceKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600, int*, int*, int, thrust::plus<int>>(int, int, int, cub::GridEvenShare<int>, thrust::plus<int>)
                    0.00%  3.8080us         1  3.8080us  3.8080us  3.8080us  void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600, int*, int*, int, thrust::plus<int>, int>(int, int, int, thrust::plus<int>, cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600)
                    0.00%  2.1120us         1  2.1120us  2.1120us  2.1120us  void cub::DeviceScanInitKernel<cub::ScanTileState<int, bool=1>>(int, int)
                    0.00%     961ns         1     961ns     961ns     961ns  [CUDA memcpy DtoH]
      API calls:   98.14%  5.45101s         2  2.72550s  1.03876s  4.41224s  cudaDeviceSynchronize
                    1.84%  102.19ms         4  25.548ms  13.844us  102.10ms  cudaMallocManaged
                    0.02%  893.35us         5  178.67us  39.549us  396.77us  cudaFree
                    0.00%  166.51us         2  83.257us  57.877us  108.64us  cudaMalloc
                    0.00%  77.436us       101     766ns     105ns  33.159us  cuDeviceGetAttribute
                    0.00%  75.389us         6  12.564us  4.2940us  30.328us  cudaLaunchKernel
                    0.00%  46.990us         3  15.663us  1.0500us  30.646us  cudaStreamSynchronize
                    0.00%  24.301us         1  24.301us  24.301us  24.301us  cudaMemcpyAsync
                    0.00%  21.091us         1  21.091us  21.091us  21.091us  cuDeviceGetName
                    0.00%  12.270us         1  12.270us  12.270us  12.270us  cuDeviceGetPCIBusId
                    0.00%  8.4830us         1  8.4830us  8.4830us  8.4830us  cudaFuncGetAttributes
                    0.00%  6.5520us         3  2.1840us     813ns  4.5290us  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
                    0.00%  5.3840us        54      99ns      91ns     239ns  cudaGetLastError
                    0.00%  4.8910us         9     543ns     212ns  2.0480us  cudaGetDevice
                    0.00%  2.1450us         3     715ns     165ns  1.6500us  cuDeviceGetCount
                    0.00%  2.1080us         3     702ns     396ns  1.1340us  cudaDeviceGetAttribute
                    0.00%     961ns         8     120ns      93ns     190ns  cudaPeekAtLastError
                    0.00%     620ns         1     620ns     620ns     620ns  cuModuleGetLoadingMode
                    0.00%     514ns         2     257ns     141ns     373ns  cuDeviceGet
                    0.00%     333ns         1     333ns     333ns     333ns  cuDeviceTotalMem
                    0.00%     227ns         1     227ns     227ns     227ns  cudaGetDeviceCount
                    0.00%     152ns         1     152ns     152ns     152ns  cuDeviceGetUuid

==69548== Unified Memory profiling result:
Device "NVIDIA GeForce GTX 1060 with Max-Q Design (0)"
   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name
     284  42.873KB  4.0000KB  0.9961MB  11.89063MB  1.132222ms  Host To Device
      85  1.9486MB  4.0000KB  2.0000MB  165.6328MB  13.45680ms  Device To Host
   23837         -         -         -           -  497.1857ms  Gpu page fault groups
Total CPU Page faults: 24
```

```shell
CUDA Kernel Statistics:

 Time (%)  Total Time (ns)  Instances     Avg (ns)         Med (ns)        Min (ns)       Max (ns)     StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  ---------------  ---------------  -------------  -------------  -----------  ----------------------------------------------------------------------------------------------------
     80.8    4,261,112,682          1  4,261,112,682.0  4,261,112,682.0  4,261,112,682  4,261,112,682          0.0  gpu_get_join_data_dynamic(int *, int *, int *, int, int, int, int *, int, int, int)                 
     19.2    1,015,684,940          1  1,015,684,940.0  1,015,684,940.0  1,015,684,940  1,015,684,940          0.0  gpu_get_join_size_per_thread(int *, int *, int, int, int, int *, int, int, int)                     
      0.0           30,592          1         30,592.0         30,592.0         30,592         30,592          0.0  void cub::DeviceScanKernel<cub::DeviceScanPolicy<int>::Policy600, int *, int *, cub::ScanTileState<…
      0.0           17,247          1         17,247.0         17,247.0         17,247         17,247          0.0  void cub::DeviceReduceKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600, …
      0.0            3,839          1          3,839.0          3,839.0          3,839          3,839          0.0  void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::P…
      0.0            2,080          1          2,080.0          2,080.0          2,080          2,080          0.0  void cub::DeviceScanInitKernel<cub::ScanTileState<int, (bool)1>>(T1, int)                           

[7/8] Executing 'gpumemtimesum' stats report

CUDA Memory Operation Statistics (by time):

 Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)              Operation            
 --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ---------------------------------
     91.0       11,550,832     74  156,092.3  160,223.0       801   198,559     25,850.0  [CUDA Unified Memory memcpy DtoH]
      9.0        1,141,517    297    3,843.5    1,279.0       831    81,664     10,266.5  [CUDA Unified Memory memcpy HtoD]
      0.0              992      1      992.0      992.0       992       992          0.0  [CUDA memcpy DtoH]               

[8/8] Executing 'gpumemsizesum' stats report

CUDA Memory Operation Statistics (by size):

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            
 ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------
    150.610     74     2.035     2.097     0.004     2.097        0.333  [CUDA Unified Memory memcpy DtoH]
     12.468    297     0.042     0.008     0.004     1.044        0.133  [CUDA Unified Memory memcpy HtoD]
      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]    
```

- Run on different configuration of `#threads` and `#blocks`
- Run on `ThetaGpu`:
```shell
qsub -A dist_relational_alg -n 1 -t 15 -q single-gpu --attrs mcdram=flat:filesystems=home -O nested_loop_out join nested_unified
```

| Number of rows | #Blocks | #Threads | #Result rows | Pass 1 | Offset calculation | Pass 2 | Total time |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 100000 | 196 | 512 | 20000986 | 0.00782237 | 0.00151229 | 0.0310354 | 0.04037 |
| 150000 | 293 | 512 | 44995231 | 0.0163467 | 0.00165638 | 0.0682299 | 0.086233 |
| 200000 | 391 | 512 | 80002265 | 0.0276299 | 0.00165044 | 0.143925 | 0.173205 |
| 250000 | 489 | 512 | 125000004 | 0.0425539 | 0.00163754 | 0.169107 | 0.213299 |
| 300000 | 586 | 512 | 179991734 | 0.0595211 | 0.00169447 | 0.225315 | 0.28653 |
| 350000 | 684 | 512 | 245006327 | 0.0798542 | 0.00169882 | 0.283174 | 0.364727 |
| 400000 | 782 | 512 | 319977044 | 0.103554 | 0.00173314 | 0.322186 | 0.427474 |
| 450000 | 879 | 512 | 404982983 | 0.129769 | 0.00179262 | 0.400548 | 0.532109 |
| 500000 | 977 | 512 | 499965209 | 0.158912 | 0.00191387 | 0.485484 | 0.64631 |
| 550000 | 1075 | 512 | 605010431 | 0.168123 | 0.00201475 | 0.558216 | 0.728353 |

Overflow at `n=600000`
```shell
GPU join operation (non-atomic): (600000, 2) x (600000, 2)
Blocks per grid: 1172, Threads per block: 512
GPU Pass 1 get join size per row in relation 1: 0.210211 seconds
Total size of the join result: -2135009041
Thrust calculate offset: 0.00207949 seconds
```
- Tried to add `cuCollections` but getting the following error:
```shell
"C:\Users\ldyke\CUDA\GPUJoin\_deps\libcudacxx-src\include\cuda\std\detail\libcxx\include\support/atomic/atomic_cuda.h( 11): fatal error C1189: #error:  "CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Window s." [C:\Users\ldyke\CUDA\GPUJoin\GPUJoin.vcxproj]" even though I have GPU with >sm_70
```

#### Nested loop join dynamic size atomic
- Added `cudaMallocManaged`.
- Run `nvprof`:
```shell
# thread 8 x 8
nvprof ./join
GPU join operation (atomic): (300000, 2) x (300000, 2)
Block dimension: (37500, 37500, 1), Thread dimension: (8, 8, 1)
==71403== NVPROF is profiling process 71403, command: ./join
Read relations: 0.0418668 seconds
GPU Pass 1 get join size per row in relation 1: 3.85068 seconds
GPU Pass 1 copy result to host: 8.3583e-05 seconds
Total size of the join result: 539975202
GPU Pass 2 join operation: 4.51339 seconds

==71403== Profiling application: ./join
==71403== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   53.96%  4.51335s         1  4.51335s  4.51335s  4.51335s  gpu_get_join_data_dynamic_atomic(int*, int*, int, int*, int, int, int, int*, int, int, int)
                   46.04%  3.85063s         1  3.85063s  3.85063s  3.85063s  gpu_get_total_join_size(int*, int, int*, int, int, int, int*, int, int, int)
                    0.00%  2.5600us         1  2.5600us  2.5600us  2.5600us  [CUDA memcpy HtoD]
                    0.00%  2.1440us         1  2.1440us  2.1440us  2.1440us  [CUDA memcpy DtoH]
      API calls:   97.78%  8.36400s         2  4.18200s  3.85064s  4.51336s  cudaDeviceSynchronize
                    1.43%  121.90ms         5  24.381ms  5.9590us  121.80ms  cudaMallocManaged
                    0.79%  67.433ms         3  22.478ms  140.75us  67.085ms  cudaFree
                    0.00%  209.75us       101  2.0760us     318ns  84.321us  cuDeviceGetAttribute
                    0.00%  97.933us         2  48.966us  16.086us  81.847us  cudaMemcpy
                    0.00%  43.277us         1  43.277us  43.277us  43.277us  cuDeviceGetName
                    0.00%  40.609us         2  20.304us  8.8210us  31.788us  cudaLaunchKernel
                    0.00%  11.987us         1  11.987us  11.987us  11.987us  cuDeviceGetPCIBusId
                    0.00%  3.2030us         3  1.0670us     506ns  2.0800us  cuDeviceGetCount
                    0.00%  1.8310us         2     915ns     317ns  1.5140us  cuDeviceGet
                    0.00%     921ns         1     921ns     921ns     921ns  cuDeviceTotalMem
                    0.00%     636ns         1     636ns     636ns     636ns  cuModuleGetLoadingMode
                    0.00%     584ns         1     584ns     584ns     584ns  cuDeviceGetUuid

==71403== Unified Memory profiling result:
Device "NVIDIA GeForce GTX 1060 with Max-Q Design (0)"
   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name
      36  130.22KB  4.0000KB  0.9961MB  4.578125MB  398.1410us  Host To Device
    6198         -         -         -           -  612.7544ms  Gpu page fault groups
Total CPU Page faults: 18
```
- Runtime error at `n=500000`: `CUDA Runtime Error: out of memory`

### Day 3 progress
- Run benchmark for 10 datasets (100000 - 550000) for Rapids (`cudf`), Pandas (`df`), our CUDA implementations
- Added cudaMemPrefetchAsync for both CUDA implementations
- Run `nsys` for same dataset for cuda implementations to optimize CUDA api calls for different `#blocks` and `#threads`
- Atomic implementation creates lots of barrier synchronization than the non atomic version
