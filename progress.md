## Intermediate result

```shell
nvcc transitive_closure.cu -run -o join
```

Benchmark for SF.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 258,428 | 223,001 | 0.0018 | 0.0039 | 0.0001 | 0.0005 |
| 2 | 297,179 | 476,246 | 0.0019 | 0.0042 | 0.0003 | 0.0007 |
| 3 | 340,027 | 755,982 | 0.0020 | 0.0046 | 0.0004 | 0.0009 |
| 4 | 388,465 | 1,056,726 | 0.0021 | 0.0053 | 0.0005 | 0.0012 |
| 5 | 443,213 | 1,376,010 | 0.0022 | 0.0075 | 0.0005 | 0.0014 |
| 6 | 504,535 | 1,711,750 | 0.0021 | 0.0063 | 0.0007 | 0.0017 |
| 7 | 573,556 | 2,062,150 | 0.0022 | 0.0070 | 0.0006 | 0.0019 |
| 8 | 650,327 | 2,426,401 | 0.0035 | 0.0077 | 0.0008 | 0.0022 |
| 9 | 735,590 | 2,803,627 | 0.0027 | 0.0082 | 0.0009 | 0.0024 |
| 10 | 829,452 | 3,193,373 | 0.0026 | 0.0088 | 0.0008 | 0.0026 |
| 11 | 932,759 | 3,595,385 | 0.0027 | 0.0094 | 0.0010 | 0.0033 |
| 12 | 1,045,446 | 4,009,951 | 0.0028 | 0.0102 | 0.0013 | 0.0035 |
| 13 | 1,167,469 | 4,437,031 | 0.0030 | 0.0121 | 0.0014 | 0.0036 |
| 14 | 1,298,788 | 4,876,068 | 0.0031 | 0.0113 | 0.0016 | 0.0043 |
| 15 | 1,439,204 | 5,326,697 | 0.0033 | 0.0120 | 0.0016 | 0.0043 |
| 16 | 1,588,293 | 5,788,461 | 0.0085 | 0.0171 | 0.0017 | 0.0049 |
| 17 | 1,745,552 | 6,260,652 | 0.0034 | 0.0141 | 0.0019 | 0.0053 |
| 18 | 1,910,098 | 6,742,695 | 0.0036 | 0.0148 | 0.0018 | 0.0056 |
| 19 | 2,082,226 | 7,233,796 | 0.0039 | 0.0198 | 0.0022 | 0.0063 |
| 20 | 2,261,690 | 7,733,688 | 0.0042 | 0.0169 | 0.0022 | 0.0067 |
| 21 | 2,448,444 | 8,241,532 | 0.0043 | 0.0178 | 0.0026 | 0.0072 |
| 22 | 2,642,134 | 8,756,848 | 0.0046 | 0.0190 | 0.0026 | 0.0074 |
| 23 | 2,842,783 | 9,279,010 | 0.0110 | 0.0236 | 0.0028 | 0.0082 |
| 24 | 3,049,532 | 9,807,603 | 0.0051 | 0.0209 | 0.0031 | 0.0083 |
| 25 | 3,262,011 | 10,341,798 | 0.0053 | 0.0218 | 0.0029 | 0.0092 |
| 26 | 3,479,859 | 10,881,222 | 0.0057 | 0.0226 | 0.0031 | 0.0098 |
| 27 | 3,702,563 | 11,425,295 | 0.0059 | 0.0239 | 0.0037 | 0.0096 |
| 28 | 3,930,141 | 11,973,638 | 0.0062 | 0.0251 | 0.0039 | 0.0103 |
| 29 | 4,162,826 | 12,526,796 | 0.0063 | 0.0259 | 0.0040 | 0.0108 |
| 30 | 4,399,940 | 13,084,624 | 0.0068 | 0.0304 | 0.0044 | 0.0109 |
| 31 | 4,639,830 | 13,646,848 | 0.0067 | 0.0277 | 0.0042 | 0.0120 |
| 32 | 4,881,763 | 14,212,245 | 0.0071 | 0.0293 | 0.0049 | 0.0126 |
| 33 | 5,125,489 | 14,779,678 | 0.0072 | 0.0323 | 0.0054 | 0.0126 |
| 34 | 5,370,961 | 15,348,483 | 0.0073 | 0.0314 | 0.0056 | 0.0138 |
| 35 | 5,617,281 | 15,918,265 | 0.0077 | 0.0320 | 0.0057 | 0.0139 |
| 36 | 5,863,799 | 16,488,145 | 0.0080 | 0.0333 | 0.0062 | 0.0142 |
| 37 | 6,110,246 | 17,057,403 | 0.0088 | 0.0342 | 0.0062 | 0.0151 |
| 38 | 6,356,681 | 17,625,807 | 0.0084 | 0.0358 | 0.0065 | 0.0154 |
| 39 | 6,602,554 | 18,193,301 | 0.0083 | 0.0362 | 0.0069 | 0.0168 |
| 40 | 6,847,726 | 18,759,980 | 0.0092 | 0.0387 | 0.0073 | 0.0169 |
| 41 | 7,092,498 | 19,325,886 | 0.0093 | 0.0386 | 0.0075 | 0.0172 |
| 42 | 7,336,065 | 19,890,746 | 0.0096 | 0.0390 | 0.0083 | 0.0175 |
| 43 | 7,578,681 | 20,454,047 | 0.0103 | 0.0407 | 0.0087 | 0.0183 |
| 44 | 7,820,628 | 21,015,866 | 0.0103 | 0.0422 | 0.0083 | 0.0193 |
| 45 | 8,061,532 | 21,576,436 | 0.0103 | 0.0427 | 0.0087 | 0.0199 |
| 46 | 8,302,092 | 22,135,333 | 0.0107 | 0.0463 | 0.0090 | 0.0194 |
| 47 | 8,542,202 | 22,693,109 | 0.0119 | 0.0479 | 0.0098 | 0.0205 |
| 48 | 8,781,537 | 23,249,591 | 0.0110 | 0.0462 | 0.0100 | 0.0212 |
| 49 | 9,019,517 | 23,804,594 | 0.0114 | 0.0542 | 0.0088 | 0.0211 |
| 50 | 9,255,908 | 24,357,862 | 0.0115 | 0.0490 | 0.0108 | 0.0215 |
| 51 | 9,490,710 | 24,909,212 | 0.0123 | 0.0499 | 0.0102 | 0.0232 |
| 52 | 9,724,827 | 25,458,554 | 0.0124 | 0.0501 | 0.0105 | 0.0236 |
| 53 | 9,957,993 | 26,006,351 | 0.0124 | 0.0518 | 0.0118 | 0.0240 |
| 54 | 10,189,846 | 26,552,384 | 0.0119 | 0.0529 | 0.0118 | 0.0232 |
| 55 | 10,419,830 | 27,096,892 | 0.0129 | 0.0536 | 0.0117 | 0.0248 |
| 56 | 10,647,646 | 27,639,482 | 0.0131 | 0.0539 | 0.0121 | 0.0244 |
| 57 | 10,873,097 | 28,179,796 | 0.0131 | 0.0552 | 0.0121 | 0.0263 |
| 58 | 11,096,484 | 28,717,697 | 0.0181 | 0.0569 | 0.0125 | 0.0256 |
| 59 | 11,318,223 | 29,253,290 | 0.0132 | 0.0572 | 0.0128 | 0.0258 |
| 60 | 11,538,193 | 29,786,407 | 0.0137 | 0.0591 | 0.0129 | 0.0271 |
| 61 | 11,755,907 | 30,316,892 | 0.0138 | 0.0593 | 0.0131 | 0.0278 |
| 62 | 11,970,992 | 30,844,793 | 0.0147 | 0.0603 | 0.0134 | 0.0280 |
| 63 | 12,183,468 | 31,369,749 | 0.0139 | 0.0623 | 0.0138 | 0.0285 |
| 64 | 12,393,580 | 31,891,466 | 0.0147 | 0.0642 | 0.0143 | 0.0288 |
| 65 | 12,601,396 | 32,409,890 | 0.0142 | 0.0632 | 0.0142 | 0.0298 |
| 66 | 12,806,514 | 32,925,490 | 0.0147 | 0.0636 | 0.0149 | 0.0306 |
| 67 | 13,008,994 | 33,437,622 | 0.0153 | 0.0649 | 0.0150 | 0.0300 |
| 68 | 13,209,201 | 33,946,228 | 0.0159 | 0.0659 | 0.0151 | 0.0313 |
| 69 | 13,406,759 | 34,451,545 | 0.0155 | 0.0686 | 0.0154 | 0.0306 |
| 70 | 13,601,867 | 34,953,666 | 0.0167 | 0.0688 | 0.0159 | 0.0334 |
| 71 | 13,794,776 | 35,452,811 | 0.0159 | 0.0711 | 0.0158 | 0.0322 |
| 72 | 13,985,408 | 35,949,111 | 0.0175 | 0.0695 | 0.0163 | 0.0324 |
| 73 | 14,173,393 | 36,442,445 | 0.0166 | 0.0702 | 0.0163 | 0.0332 |
| 74 | 14,358,843 | 36,932,782 | 0.0167 | 0.0723 | 0.0170 | 0.0334 |
| 75 | 14,542,560 | 37,420,101 | 0.0193 | 0.0734 | 0.0167 | 0.0342 |
| 76 | 14,724,948 | 37,904,568 | 0.0172 | 0.0728 | 0.0171 | 0.0344 |
| 77 | 14,905,832 | 38,386,437 | 0.0173 | 0.0744 | 0.0172 | 0.0346 |
| 78 | 15,085,380 | 38,865,648 | 0.0170 | 0.0747 | 0.0177 | 0.0342 |
| 79 | 15,263,368 | 39,341,838 | 0.0193 | 0.0762 | 0.0179 | 0.0354 |
| 80 | 15,439,408 | 39,814,740 | 0.0172 | 0.0775 | 0.0181 | 0.0360 |
| 81 | 15,613,398 | 40,284,305 | 0.0171 | 0.0774 | 0.0185 | 0.0364 |
| 82 | 15,785,334 | 40,750,481 | 0.0181 | 0.0782 | 0.0188 | 0.0363 |
| 83 | 15,955,256 | 41,213,907 | 0.0183 | 0.0799 | 0.0187 | 0.0375 |
| 84 | 16,123,566 | 41,674,629 | 0.0182 | 0.0802 | 0.0194 | 0.0375 |
| 85 | 16,290,220 | 42,132,970 | 0.0179 | 0.0805 | 0.0191 | 0.0387 |
| 86 | 16,455,649 | 42,589,055 | 0.0183 | 0.0836 | 0.0194 | 0.0396 |
| 87 | 16,619,166 | 43,042,721 | 0.0186 | 0.0849 | 0.0192 | 0.0389 |
| 88 | 16,780,634 | 43,493,531 | 0.0191 | 0.0840 | 0.0188 | 0.0390 |
| 89 | 16,939,995 | 43,941,165 | 0.0187 | 0.0898 | 0.0194 | 0.0407 |
| 90 | 17,097,292 | 44,385,331 | 0.0191 | 0.0860 | 0.0200 | 0.0391 |
| 91 | 17,252,053 | 44,826,120 | 0.0191 | 0.0867 | 0.0202 | 0.0404 |
| 92 | 17,404,562 | 45,263,361 | 0.0194 | 0.0869 | 0.0206 | 0.0400 |
| 93 | 17,555,219 | 45,697,225 | 0.0195 | 0.0884 | 0.0208 | 0.0416 |
| 94 | 17,705,057 | 46,127,985 | 0.0196 | 0.0883 | 0.0207 | 0.0425 |
| 95 | 17,853,671 | 46,556,237 | 0.0198 | 0.0891 | 0.0212 | 0.0417 |
| 96 | 18,000,988 | 46,981,573 | 0.0207 | 0.0914 | 0.0205 | 0.0434 |
| 97 | 18,146,687 | 47,403,868 | 0.0270 | 0.0946 | 0.0207 | 0.0442 |
| 98 | 18,290,708 | 47,822,709 | 0.0214 | 0.0922 | 0.0216 | 0.0450 |
| 99 | 18,432,995 | 48,238,000 | 0.0205 | 0.0936 | 0.0214 | 0.0447 |
| 100 | 18,573,442 | 48,649,760 | 0.0208 | 0.0939 | 0.0217 | 0.0440 |
| 101 | 18,712,279 | 49,058,284 | 0.0212 | 0.0952 | 0.0221 | 0.0462 |
| 102 | 18,849,737 | 49,463,876 | 0.0206 | 0.0956 | 0.0223 | 0.0463 |
| 103 | 18,985,676 | 49,866,997 | 0.0209 | 0.0955 | 0.0220 | 0.0459 |
| 104 | 19,119,931 | 50,267,588 | 0.0213 | 0.0973 | 0.0222 | 0.0478 |
| 105 | 19,253,010 | 50,665,511 | 0.0215 | 0.0971 | 0.0222 | 0.0467 |
| 106 | 19,384,369 | 51,061,106 | 0.0211 | 0.0975 | 0.0220 | 0.0468 |
| 107 | 19,513,758 | 51,454,139 | 0.0218 | 0.0992 | 0.0226 | 0.0468 |
| 108 | 19,641,336 | 51,844,140 | 0.0215 | 0.0997 | 0.0233 | 0.0489 |
| 109 | 19,767,208 | 52,231,327 | 0.0209 | 0.1005 | 0.0226 | 0.0486 |
| 110 | 19,891,769 | 52,615,475 | 0.0211 | 0.1008 | 0.0232 | 0.0482 |
| 111 | 20,015,281 | 52,996,813 | 0.0215 | 0.1014 | 0.0231 | 0.0489 |
| 112 | 20,137,769 | 53,375,817 | 0.0218 | 0.1015 | 0.0238 | 0.0495 |
| 113 | 20,258,768 | 53,752,812 | 0.0217 | 0.1036 | 0.0236 | 0.0509 |
| 114 | 20,378,203 | 54,127,794 | 0.0223 | 0.1037 | 0.0239 | 0.0502 |
| 115 | 20,496,578 | 54,500,796 | 0.0222 | 0.1037 | 0.0243 | 0.0515 |
| 116 | 20,613,881 | 54,872,132 | 0.0226 | 0.1048 | 0.0241 | 0.0510 |
| 117 | 20,730,849 | 55,241,914 | 0.0225 | 0.1055 | 0.0249 | 0.0509 |
| 118 | 20,847,483 | 55,610,441 | 0.0226 | 0.1066 | 0.0243 | 0.0510 |
| 119 | 20,963,966 | 55,977,545 | 0.0222 | 0.1073 | 0.0251 | 0.0516 |
| 120 | 21,080,057 | 56,343,282 | 0.0224 | 0.1082 | 0.0243 | 0.0511 |
| 121 | 21,196,025 | 56,707,575 | 0.0225 | 0.1076 | 0.0246 | 0.0519 |
| 122 | 21,311,392 | 57,070,595 | 0.0230 | 0.1104 | 0.0255 | 0.0528 |
| 123 | 21,425,729 | 57,431,978 | 0.0240 | 0.1093 | 0.0250 | 0.0536 |
| 124 | 21,539,122 | 57,791,689 | 0.0230 | 0.1108 | 0.0255 | 0.0538 |
| 125 | 21,651,407 | 58,150,108 | 0.0229 | 0.1134 | 0.0259 | 0.0528 |
| 126 | 21,763,206 | 58,507,184 | 0.0241 | 0.1115 | 0.0255 | 0.0553 |
| 127 | 21,875,190 | 58,863,245 | 0.0235 | 0.1129 | 0.0260 | 0.0536 |
| 128 | 21,987,742 | 59,218,664 | 0.0235 | 0.1125 | 0.0259 | 0.0550 |
| 129 | 22,101,342 | 59,573,836 | 0.0237 | 0.1140 | 0.0259 | 0.0567 |
| 130 | 22,215,770 | 59,929,167 | 0.0237 | 0.1158 | 0.0260 | 0.0552 |
| 131 | 22,330,698 | 60,284,508 | 0.0242 | 0.1190 | 0.0221 | 0.0558 |
| 132 | 22,446,155 | 60,639,440 | 0.0238 | 0.1162 | 0.0266 | 0.0551 |
| 133 | 22,562,021 | 60,994,007 | 0.0245 | 0.1163 | 0.0267 | 0.0562 |
| 134 | 22,678,439 | 61,348,377 | 0.0244 | 0.1171 | 0.0268 | 0.0570 |
| 135 | 22,795,195 | 61,702,405 | 0.0243 | 0.1175 | 0.0269 | 0.0567 |
| 136 | 22,911,868 | 62,055,865 | 0.0250 | 0.1187 | 0.0270 | 0.0569 |
| 137 | 23,028,300 | 62,408,260 | 0.0279 | 0.1195 | 0.0270 | 0.0577 |
| 138 | 23,143,861 | 62,759,153 | 0.0253 | 0.1191 | 0.0277 | 0.0575 |
| 139 | 23,258,109 | 63,107,949 | 0.0250 | 0.1211 | 0.0276 | 0.0592 |
| 140 | 23,371,363 | 63,454,403 | 0.0247 | 0.1215 | 0.0281 | 0.0576 |
| 141 | 23,483,834 | 63,798,667 | 0.0252 | 0.1219 | 0.0279 | 0.0602 |
| 142 | 23,596,378 | 64,141,120 | 0.0253 | 0.1225 | 0.0285 | 0.0588 |
| 143 | 23,708,639 | 64,482,480 | 0.0344 | 0.1217 | 0.0285 | 0.0589 |
| 144 | 23,820,573 | 64,822,707 | 0.0260 | 0.1233 | 0.0287 | 0.0595 |
| 145 | 23,931,864 | 65,161,915 | 0.0252 | 0.1234 | 0.0282 | 0.0600 |
| 146 | 24,041,537 | 65,499,978 | 0.0254 | 0.1232 | 0.0287 | 0.0611 |
| 147 | 24,149,532 | 65,836,150 | 0.0260 | 0.1255 | 0.0286 | 0.0595 |
| 148 | 24,256,198 | 66,170,073 | 0.0263 | 0.1267 | 0.0290 | 0.0604 |
| 149 | 24,361,761 | 66,502,122 | 0.0260 | 0.1274 | 0.0294 | 0.0621 |
| 150 | 24,465,788 | 66,832,236 | 0.0263 | 0.1309 | 0.0288 | 0.0633 |
| 151 | 24,568,591 | 67,159,849 | 0.0259 | 0.1283 | 0.0298 | 0.0622 |
| 152 | 24,669,140 | 67,485,035 | 0.0261 | 0.1291 | 0.0289 | 0.0625 |
| 153 | 24,766,887 | 67,806,862 | 0.0260 | 0.1298 | 0.0299 | 0.0621 |
| 154 | 24,861,456 | 68,124,852 | 0.0259 | 0.1343 | 0.0260 | 0.0631 |
| 155 | 24,952,883 | 68,438,764 | 0.0264 | 0.1301 | 0.0306 | 0.0635 |
| 156 | 25,041,053 | 68,748,701 | 0.0265 | 0.1355 | 0.0270 | 0.0634 |
| 157 | 25,126,531 | 69,054,594 | 0.0269 | 0.1391 | 0.0300 | 0.0627 |
| 158 | 25,209,434 | 69,356,883 | 0.0266 | 0.1319 | 0.0307 | 0.0643 |
| 159 | 25,289,551 | 69,655,712 | 0.0273 | 0.1333 | 0.0302 | 0.0626 |
| 160 | 25,366,851 | 69,950,812 | 0.0275 | 0.1324 | 0.0310 | 0.0665 |
| 161 | 25,440,753 | 70,241,914 | 0.0269 | 0.1330 | 0.0310 | 0.0635 |
| 162 | 25,511,333 | 70,528,877 | 0.0274 | 0.1338 | 0.0308 | 0.0650 |
| 163 | 25,578,588 | 70,811,829 | 0.0272 | 0.1361 | 0.0295 | 0.0645 |
| 164 | 25,642,294 | 71,090,923 | 0.0284 | 0.1706 | 0.0312 | 0.0652 |
| 165 | 25,702,377 | 71,366,182 | 0.0591 | 0.1447 | 0.0490 | 0.0717 |
| 166 | 25,758,772 | 71,637,611 | 0.0278 | 0.1361 | 0.0314 | 0.0652 |
| 167 | 25,811,595 | 71,905,185 | 0.0280 | 0.1352 | 0.0316 | 0.0677 |
| 168 | 25,860,827 | 72,168,945 | 0.0272 | 0.1356 | 0.0313 | 0.0678 |
| 169 | 25,906,116 | 72,428,653 | 0.0281 | 0.1373 | 0.0310 | 0.0663 |
| 170 | 25,947,522 | 72,684,006 | 0.0277 | 0.1373 | 0.0315 | 0.0671 |
| 171 | 25,985,108 | 72,934,972 | 0.0278 | 0.1369 | 0.0313 | 0.0680 |
| 172 | 26,019,122 | 73,181,708 | 0.0280 | 0.1374 | 0.0316 | 0.0669 |
| 173 | 26,049,941 | 73,424,676 | 0.0274 | 0.1390 | 0.0314 | 0.0686 |
| 174 | 26,077,750 | 73,664,310 | 0.0276 | 0.1414 | 0.0314 | 0.0678 |
| 175 | 26,102,923 | 73,900,521 | 0.0280 | 0.1395 | 0.0317 | 0.0683 |
| 176 | 26,125,268 | 74,133,461 | 0.0281 | 0.1389 | 0.0316 | 0.0673 |
| 177 | 26,144,218 | 74,363,061 | 0.0362 | 0.1488 | 0.0285 | 0.0679 |
| 178 | 26,159,701 | 74,588,756 | 0.0283 | 0.1403 | 0.0314 | 0.0683 |
| 179 | 26,170,842 | 74,810,673 | 0.0282 | 0.1404 | 0.0323 | 0.0692 |
| 180 | 26,177,418 | 75,027,937 | 0.0274 | 0.1407 | 0.0313 | 0.0698 |
| 181 | 26,179,328 | 75,240,140 | 0.0285 | 0.1417 | 0.0318 | 0.0673 |
| 182 | 26,176,571 | 75,446,962 | 0.0281 | 0.1417 | 0.0316 | 0.0672 |
| 183 | 26,169,282 | 75,648,307 | 0.0276 | 0.1421 | 0.0318 | 0.0693 |
| 184 | 26,157,702 | 75,844,252 | 0.0280 | 0.1418 | 0.0321 | 0.0685 |
| 185 | 26,141,497 | 76,034,789 | 0.0281 | 0.1440 | 0.0318 | 0.0692 |
| 186 | 26,120,708 | 76,219,965 | 0.0280 | 0.1428 | 0.0320 | 0.0686 |
| 187 | 26,094,887 | 76,399,917 | 0.0275 | 0.1430 | 0.0312 | 0.0704 |
| 188 | 26,064,180 | 76,574,662 | 0.0282 | 0.1436 | 0.0317 | 0.0712 |
| 189 | 26,028,824 | 76,744,231 | 0.0280 | 0.1434 | 0.0315 | 0.0697 |
| 190 | 25,989,032 | 76,908,857 | 0.0282 | 0.1444 | 0.0319 | 0.0707 |
| 191 | 25,944,531 | 77,068,533 | 0.0279 | 0.1444 | 0.0314 | 0.0691 |
| 192 | 25,895,518 | 77,223,086 | 0.0277 | 0.1447 | 0.0312 | 0.0681 |
| 193 | 25,841,541 | 77,372,718 | 0.0273 | 0.1441 | 0.0316 | 0.0700 |
| 194 | 25,782,784 | 77,517,227 | 0.0275 | 0.1475 | 0.0311 | 0.0703 |
| 195 | 25,719,461 | 77,656,631 | 0.0267 | 0.1438 | 0.0314 | 0.0701 |
| 196 | 25,651,566 | 77,791,065 | 0.0269 | 0.1442 | 0.0312 | 0.0720 |
| 197 | 25,579,143 | 77,920,428 | 0.0270 | 0.1452 | 0.0314 | 0.0699 |
| 198 | 25,502,630 | 78,044,726 | 0.0261 | 0.1440 | 0.0301 | 0.0695 |
| 199 | 25,422,369 | 78,164,252 | 0.0277 | 0.1480 | 0.0318 | 0.0712 |
| 200 | 25,338,487 | 78,279,222 | 0.0270 | 0.1451 | 0.0311 | 0.0690 |
| 201 | 25,251,711 | 78,389,839 | 0.0265 | 0.1467 | 0.0302 | 0.0703 |
| 202 | 25,162,254 | 78,496,604 | 0.0272 | 0.1453 | 0.0305 | 0.0715 |
| 203 | 25,070,487 | 78,599,703 | 0.0272 | 0.1449 | 0.0301 | 0.0713 |
| 204 | 24,976,370 | 78,699,354 | 0.0264 | 0.1455 | 0.0297 | 0.0719 |
| 205 | 24,879,582 | 78,795,763 | 0.0269 | 0.1465 | 0.0301 | 0.0698 |
| 206 | 24,780,091 | 78,888,736 | 0.0280 | 0.1502 | 0.0291 | 0.0700 |
| 207 | 24,677,742 | 78,978,233 | 0.0272 | 0.1482 | 0.0287 | 0.0695 |
| 208 | 24,572,261 | 79,064,276 | 0.0271 | 0.1474 | 0.0287 | 0.0707 |
| 209 | 24,463,663 | 79,146,714 | 0.0258 | 0.1456 | 0.0295 | 0.0702 |
| 210 | 24,351,844 | 79,225,481 | 0.0256 | 0.1472 | 0.0298 | 0.0710 |
| 211 | 24,236,872 | 79,300,584 | 0.0256 | 0.1508 | 0.0248 | 0.0719 |
| 212 | 24,118,998 | 79,372,082 | 0.0261 | 0.1462 | 0.0287 | 0.0704 |
| 213 | 23,998,051 | 79,440,053 | 0.0250 | 0.1462 | 0.0287 | 0.0700 |
| 214 | 23,874,380 | 79,504,504 | 0.0252 | 0.1481 | 0.0284 | 0.0700 |
| 215 | 23,748,149 | 79,565,625 | 0.0251 | 0.1433 | 0.0286 | 0.0704 |
| 216 | 23,619,917 | 79,623,487 | 0.0243 | 0.1493 | 0.0281 | 0.0705 |
| 217 | 23,489,657 | 79,678,253 | 0.0271 | 0.1446 | 0.0282 | 0.0709 |
| 218 | 23,357,630 | 79,730,071 | 0.0258 | 0.1459 | 0.0287 | 0.0688 |
| 219 | 23,223,665 | 79,779,016 | 0.0247 | 0.1460 | 0.0282 | 0.0699 |
| 220 | 23,087,883 | 79,825,181 | 0.0242 | 0.1461 | 0.0276 | 0.0688 |
| 221 | 22,950,355 | 79,868,670 | 0.0243 | 0.1452 | 0.0272 | 0.0686 |
| 222 | 22,810,876 | 79,909,515 | 0.0242 | 0.1449 | 0.0271 | 0.0687 |
| 223 | 22,670,014 | 79,947,770 | 0.0241 | 0.1445 | 0.0274 | 0.0694 |
| 224 | 22,527,520 | 79,983,633 | 0.0236 | 0.1458 | 0.0265 | 0.0680 |
| 225 | 22,383,535 | 80,017,213 | 0.0236 | 0.1449 | 0.0264 | 0.0694 |
| 226 | 22,238,007 | 80,048,760 | 0.0231 | 0.1444 | 0.0272 | 0.0702 |
| 227 | 22,090,965 | 80,078,463 | 0.0234 | 0.1443 | 0.0264 | 0.0688 |
| 228 | 21,942,371 | 80,106,504 | 0.0235 | 0.1441 | 0.0260 | 0.0689 |
| 229 | 21,792,483 | 80,133,134 | 0.0244 | 0.1461 | 0.0261 | 0.0686 |
| 230 | 21,641,274 | 80,158,393 | 0.0233 | 0.1436 | 0.0256 | 0.0672 |
| 231 | 21,488,719 | 80,182,374 | 0.0226 | 0.1441 | 0.0257 | 0.0678 |
| 232 | 21,335,006 | 80,205,090 | 0.0260 | 0.1461 | 0.0250 | 0.0691 |
| 233 | 21,180,264 | 80,226,530 | 0.0228 | 0.1433 | 0.0250 | 0.0678 |
| 234 | 21,024,345 | 80,246,765 | 0.0229 | 0.1435 | 0.0249 | 0.0687 |
| 235 | 20,867,254 | 80,265,670 | 0.0229 | 0.1447 | 0.0236 | 0.0702 |
| 236 | 20,709,354 | 80,283,284 | 0.0225 | 0.1435 | 0.0246 | 0.0680 |
| 237 | 20,551,039 | 80,299,769 | 0.0216 | 0.1445 | 0.0252 | 0.0681 |
| 238 | 20,392,609 | 80,315,319 | 0.0223 | 0.1433 | 0.0246 | 0.0683 |
| 239 | 20,234,025 | 80,330,019 | 0.0218 | 0.1445 | 0.0238 | 0.0681 |
| 240 | 20,075,387 | 80,343,919 | 0.0215 | 0.1446 | 0.0231 | 0.0689 |
| 241 | 19,916,531 | 80,357,024 | 0.0229 | 0.1437 | 0.0235 | 0.0675 |
| 242 | 19,757,363 | 80,369,267 | 0.0218 | 0.1493 | 0.0239 | 0.0663 |
| 243 | 19,597,812 | 80,380,641 | 0.0217 | 0.1419 | 0.0230 | 0.0682 |
| 244 | 19,438,135 | 80,391,114 | 0.0219 | 0.1425 | 0.0238 | 0.0686 |
| 245 | 19,278,237 | 80,400,766 | 0.0232 | 0.1432 | 0.0225 | 0.0683 |
| 246 | 19,118,218 | 80,409,684 | 0.0220 | 0.2057 | 0.0186 | 0.0685 |
| 247 | 18,958,400 | 80,417,910 | 0.0207 | 0.1421 | 0.0222 | 0.0672 |
| 248 | 18,798,681 | 80,425,562 | 0.0204 | 0.1411 | 0.0219 | 0.0678 |
| 249 | 18,639,111 | 80,432,654 | 0.0209 | 0.1406 | 0.0222 | 0.0654 |
| 250 | 18,479,683 | 80,439,173 | 0.0202 | 0.1425 | 0.0216 | 0.0666 |
| 251 | 18,320,270 | 80,445,155 | 0.0203 | 0.1406 | 0.0214 | 0.0667 |
| 252 | 18,160,756 | 80,450,578 | 0.0202 | 0.1420 | 0.0221 | 0.0668 |
| 253 | 18,001,345 | 80,455,469 | 0.0197 | 0.1423 | 0.0216 | 0.0666 |
| 254 | 17,841,981 | 80,459,897 | 0.0201 | 0.1449 | 0.0175 | 0.0659 |
| 255 | 17,682,900 | 80,463,897 | 0.0243 | 0.1413 | 0.0206 | 0.0653 |
| 256 | 17,524,093 | 80,467,539 | 0.0197 | 0.1424 | 0.0207 | 0.0664 |
| 257 | 17,365,486 | 80,470,869 | 0.0187 | 0.1408 | 0.0209 | 0.0662 |
| 258 | 17,207,108 | 80,473,927 | 0.0189 | 0.1408 | 0.0203 | 0.0655 |
| 259 | 17,049,076 | 80,476,772 | 0.0191 | 0.1400 | 0.0203 | 0.0665 |
| 260 | 16,891,512 | 80,479,391 | 0.0190 | 0.1411 | 0.0204 | 0.0653 |
| 261 | 16,734,574 | 80,481,772 | 0.0183 | 0.1405 | 0.0194 | 0.0669 |
| 262 | 16,578,492 | 80,483,924 | 0.0186 | 0.1404 | 0.0199 | 0.0653 |
| 263 | 16,423,170 | 80,485,843 | 0.0195 | 0.1408 | 0.0191 | 0.0660 |
| 264 | 16,268,502 | 80,487,531 | 0.0183 | 0.1414 | 0.0186 | 0.0650 |
| 265 | 16,114,173 | 80,489,028 | 0.0179 | 0.1409 | 0.0192 | 0.0643 |
| 266 | 15,960,159 | 80,490,346 | 0.0188 | 0.1391 | 0.0192 | 0.0660 |
| 267 | 15,806,341 | 80,491,511 | 0.0188 | 0.1450 | 0.0194 | 0.0651 |
| 268 | 15,652,613 | 80,492,545 | 0.0190 | 0.1417 | 0.0187 | 0.0648 |
| 269 | 15,498,985 | 80,493,456 | 0.0177 | 0.1946 | 0.0187 | 0.0652 |
| 270 | 15,345,338 | 80,494,260 | 0.0189 | 0.1394 | 0.0533 | 0.1003 |
| 271 | 15,191,697 | 80,494,968 | 0.0175 | 0.1392 | 0.0184 | 0.0663 |
| 272 | 15,037,983 | 80,495,572 | 0.0170 | 0.1387 | 0.0179 | 0.0655 |
| 273 | 14,884,368 | 80,496,081 | 0.0179 | 0.1392 | 0.0180 | 0.0638 |
| 274 | 14,731,165 | 80,496,493 | 0.0181 | 0.1387 | 0.0172 | 0.0645 |
| 275 | 14,578,412 | 80,496,825 | 0.0178 | 0.1394 | 0.0177 | 0.0645 |
| 276 | 14,426,338 | 80,497,095 | 0.0164 | 0.1395 | 0.0171 | 0.0647 |
| 277 | 14,274,975 | 80,497,309 | 0.0168 | 0.1363 | 0.0174 | 0.0631 |
| 278 | 14,124,386 | 80,497,473 | 0.0172 | 0.1399 | 0.0158 | 0.0650 |
| 279 | 13,974,676 | 80,497,614 | 0.0161 | 0.1385 | 0.0165 | 0.0654 |
| 280 | 13,826,024 | 80,497,730 | 0.0157 | 0.1380 | 0.0170 | 0.0633 |
| 281 | 13,678,425 | 80,497,823 | 0.0169 | 0.1387 | 0.0160 | 0.0642 |
| 282 | 13,532,088 | 80,497,895 | 0.0158 | 0.1372 | 0.0168 | 0.0635 |
| 283 | 13,386,931 | 80,497,948 | 0.0168 | 0.1370 | 0.0155 | 0.0644 |
| 284 | 13,242,862 | 80,497,984 | 0.0155 | 0.1375 | 0.0161 | 0.0631 |
| 285 | 13,099,988 | 80,498,003 | 0.0154 | 0.2923 | 0.0156 | 0.0642 |
| 286 | 12,958,176 | 80,498,011 | 0.0173 | 0.2802 | 0.0150 | 0.0633 |
| 287 | 12,817,350 | 80,498,014 | 0.0190 | 0.1398 | 0.0148 | 0.0634 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| SF.cedge | 223,001 | 80,498,014 | 287 | 3,456 x 1,024 | 70.1464 |


Initialization: 1.3879, Read: 0.0424, reverse: 0.0008
Hashtable rate: 6,406,970,062 keys/s, time: 0.0000
Join: 5.6243
Projection: 5.9024
Deduplication: 30.2819
Memory clear: 12.7535
Union: 14.1532
Total: 70.1464

Benchmark for p2p-Gnutella09
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 105,493 | 26,013 | 0.0017 | 0.0032 | 0.0002 | 0.0004 |
| 2 | 404,904 | 129,548 | 0.0018 | 0.0041 | 0.0005 | 0.0006 |
| 3 | 1,325,000 | 505,177 | 0.0025 | 0.0057 | 0.0013 | 0.0013 |
| 4 | 3,600,153 | 1,621,937 | 0.0052 | 0.0109 | 0.0036 | 0.0038 |
| 5 | 7,770,985 | 4,262,546 | 0.0094 | 0.0202 | 0.0089 | 0.0076 |
| 6 | 12,779,550 | 8,761,434 | 0.0183 | 0.0337 | 0.0176 | 0.0145 |
| 7 | 16,677,166 | 13,700,045 | 0.0277 | 0.0476 | 0.0250 | 0.0213 |
| 8 | 18,909,848 | 17,258,403 | 0.0366 | 0.0564 | 0.0289 | 0.0255 |
| 9 | 20,042,582 | 19,214,882 | 0.0403 | 0.0614 | 0.0314 | 0.0259 |
| 10 | 20,657,402 | 20,204,059 | 0.0435 | 0.0634 | 0.0330 | 0.0291 |
| 11 | 21,009,215 | 20,751,684 | 0.0430 | 0.0649 | 0.0337 | 0.0294 |
| 12 | 21,211,485 | 21,064,266 | 0.0433 | 0.0664 | 0.0339 | 0.0298 |
| 13 | 21,322,255 | 21,243,127 | 0.0448 | 0.2135 | 0.0327 | 0.0321 |
| 14 | 21,375,212 | 21,339,158 | 0.0451 | 0.0662 | 0.0345 | 0.0293 |
| 15 | 21,394,456 | 21,382,486 | 0.0442 | 0.2131 | 0.0329 | 0.0331 |
| 16 | 21,400,317 | 21,397,410 | 0.0468 | 0.3048 | 0.0331 | 0.0336 |
| 17 | 21,401,624 | 21,401,928 | 0.0565 | 0.2850 | 0.0338 | 0.0387 |
| 18 | 21,401,798 | 21,402,851 | 0.0594 | 0.2779 | 0.0334 | 0.0442 |
| 19 | 21,401,809 | 21,402,957 | 0.0581 | 0.2741 | 0.0332 | 0.0465 |
| 20 | 21,401,809 | 21,402,960 | 0.0574 | 0.2731 | 0.0336 | 0.0481 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| p2p-Gnutella09 | 26,013 | 21,402,960 | 20 | 3,456 x 1,024 | 4.7777 |


Initialization: 0.0005, Read: 0.0113, reverse: 0.0003
Hashtable rate: 462,733,029 keys/s, time: 0.0001
Join: 0.6856
Projection: 0.4852
Deduplication: 2.3455
Memory clear: 0.7543
Union: 0.4948
Total: 4.7777

Benchmark for p2p-Gnutella04
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 179,268 | 39,994 | 0.0020 | 0.0035 | 0.0002 | 0.0004 |
| 2 | 774,471 | 218,370 | 0.0019 | 0.0093 | 0.0011 | 0.0009 |
| 3 | 3,098,417 | 976,616 | 0.0036 | 0.0093 | 0.0033 | 0.0030 |
| 4 | 9,923,248 | 3,842,060 | 0.0090 | 0.0221 | 0.0112 | 0.0096 |
| 5 | 21,660,016 | 11,653,146 | 0.0258 | 0.0516 | 0.0292 | 0.0230 |
| 6 | 32,626,065 | 23,748,922 | 0.0553 | 0.3567 | 0.0561 | 0.1134 |
| 7 | 39,357,207 | 34,088,772 | 0.1066 | 0.4984 | 0.0348 | 0.1706 |
| 8 | 42,840,923 | 40,152,911 | 0.1278 | 0.5193 | 0.0699 | 0.1531 |
| 9 | 44,597,885 | 43,246,096 | 0.1303 | 0.5769 | 0.0690 | 0.1332 |
| 10 | 45,562,373 | 44,812,737 | 0.1308 | 0.5953 | 0.0751 | 0.1265 |
| 11 | 46,149,143 | 45,689,891 | 0.1368 | 0.6055 | 0.0759 | 0.1216 |
| 12 | 46,495,830 | 46,226,844 | 0.1380 | 0.6126 | 0.0776 | 0.1193 |
| 13 | 46,686,382 | 46,539,051 | 0.1360 | 0.6274 | 0.0773 | 0.1167 |
| 14 | 46,798,445 | 46,710,511 | 0.1371 | 0.6276 | 0.0779 | 0.1118 |
| 15 | 46,872,117 | 46,813,927 | 0.1377 | 0.6276 | 0.0790 | 0.1141 |
| 16 | 46,927,744 | 46,883,084 | 0.1381 | 0.6262 | 0.0780 | 0.1171 |
| 17 | 46,976,810 | 46,937,127 | 0.1382 | 0.6283 | 0.0777 | 0.1125 |
| 18 | 47,017,268 | 46,984,995 | 0.1376 | 0.6294 | 0.0780 | 0.1154 |
| 19 | 47,042,974 | 47,023,338 | 0.1391 | 0.6272 | 0.0784 | 0.1130 |
| 20 | 47,054,749 | 47,046,747 | 0.1393 | 0.6305 | 0.0782 | 0.1123 |
| 21 | 47,057,636 | 47,056,798 | 0.1417 | 0.6315 | 0.0790 | 0.1144 |
| 22 | 47,058,085 | 47,059,086 | 0.1409 | 0.6330 | 0.0780 | 0.1118 |
| 23 | 47,058,153 | 47,059,447 | 0.1389 | 0.6333 | 0.0782 | 0.1147 |
| 24 | 47,058,172 | 47,059,508 | 0.1439 | 0.6363 | 0.0789 | 0.1124 |
| 25 | 47,058,176 | 47,059,523 | 0.1402 | 0.6355 | 0.0782 | 0.1124 |
| 26 | 47,058,176 | 47,059,527 | 0.1398 | 0.6355 | 0.0779 | 0.1140 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| p2p-Gnutella04 | 39,994 | 47,059,527 | 26 | 3,456 x 1,024 | 22.1751 |


Initialization: 0.0008, Read: 0.0079, reverse: 0.0006
Hashtable rate: 527,806,371 keys/s, time: 0.0001
Join: 2.8165
Projection: 1.5980
Deduplication: 12.6899
Memory clear: 2.4938
Union: 2.5674
Total: 22.1751

Benchmark for cal.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 19,835 | 21,693 | 0.0019 | 0.0034 | 0.0001 | 0.0002 |
| 2 | 18,616 | 41,526 | 0.0015 | 0.0032 | 0.0000 | 0.0001 |
| 3 | 17,604 | 60,132 | 0.0015 | 0.0031 | 0.0000 | 0.0002 |
| 4 | 16,703 | 77,715 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 5 | 15,876 | 94,383 | 0.0015 | 0.0032 | 0.0000 | 0.0002 |
| 6 | 15,118 | 110,206 | 0.0015 | 0.0034 | 0.0000 | 0.0000 |
| 7 | 14,398 | 125,261 | 0.0015 | 0.0037 | 0.0000 | 0.0000 |
| 8 | 13,712 | 139,584 | 0.0015 | 0.0034 | 0.0000 | 0.0000 |
| 9 | 13,048 | 153,218 | 0.0015 | 0.0036 | 0.0000 | 0.0000 |
| 10 | 12,415 | 166,192 | 0.0015 | 0.0036 | 0.0000 | 0.0000 |
| 11 | 11,825 | 178,538 | 0.0015 | 0.0034 | 0.0000 | 0.0006 |
| 12 | 11,283 | 190,304 | 0.0015 | 0.0035 | 0.0000 | 0.0003 |
| 13 | 10,773 | 201,538 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 14 | 10,305 | 212,277 | 0.0015 | 0.0033 | 0.0000 | 0.0003 |
| 15 | 9,852 | 222,554 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 16 | 9,431 | 232,384 | 0.0034 | 0.0035 | 0.0000 | 0.0003 |
| 17 | 9,032 | 241,797 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 18 | 8,657 | 250,810 | 0.0015 | 0.0033 | 0.0000 | 0.0003 |
| 19 | 8,313 | 259,449 | 0.0015 | 0.0035 | 0.0000 | 0.0004 |
| 20 | 7,985 | 267,745 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 21 | 7,668 | 275,713 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 22 | 7,361 | 283,363 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 23 | 7,079 | 290,703 | 0.0015 | 0.0036 | 0.0000 | 0.0003 |
| 24 | 6,818 | 297,762 | 0.0015 | 0.0036 | 0.0000 | 0.0004 |
| 25 | 6,568 | 304,561 | 0.0015 | 0.0037 | 0.0000 | 0.0004 |
| 26 | 6,335 | 311,114 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 27 | 6,113 | 317,434 | 0.0015 | 0.0037 | 0.0000 | 0.0004 |
| 28 | 5,891 | 323,532 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 29 | 5,680 | 329,409 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 30 | 5,477 | 335,075 | 0.0015 | 0.0040 | 0.0000 | 0.0005 |
| 31 | 5,283 | 340,538 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 32 | 5,102 | 345,808 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 33 | 4,933 | 350,899 | 0.0015 | 0.0037 | 0.0000 | 0.0008 |
| 34 | 4,762 | 355,820 | 0.0015 | 0.0145 | 0.0000 | 0.0005 |
| 35 | 4,595 | 360,571 | 0.0116 | 0.0187 | 0.0000 | 0.0005 |
| 36 | 4,438 | 365,155 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 37 | 4,283 | 369,583 | 0.0015 | 0.0037 | 0.0000 | 0.0007 |
| 38 | 4,134 | 373,855 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 39 | 3,989 | 377,981 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 40 | 3,856 | 381,964 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 41 | 3,723 | 385,813 | 0.0016 | 0.0037 | 0.0000 | 0.0005 |
| 42 | 3,596 | 389,531 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 43 | 3,477 | 393,123 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 44 | 3,355 | 396,594 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 45 | 3,226 | 399,943 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 46 | 3,106 | 403,163 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 47 | 2,987 | 406,263 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 48 | 2,879 | 409,243 | 0.0015 | 0.0037 | 0.0000 | 0.0005 |
| 49 | 2,782 | 412,116 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 50 | 2,691 | 414,893 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 51 | 2,601 | 417,579 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 52 | 2,515 | 420,175 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 53 | 2,432 | 422,685 | 0.0032 | 0.0038 | 0.0000 | 0.0009 |
| 54 | 2,346 | 425,112 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 55 | 2,264 | 427,454 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 56 | 2,190 | 429,715 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 57 | 2,120 | 431,902 | 0.0015 | 0.0040 | 0.0000 | 0.0008 |
| 58 | 2,052 | 434,019 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 59 | 1,984 | 436,068 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 60 | 1,920 | 438,049 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 61 | 1,855 | 439,966 | 0.0014 | 0.0041 | 0.0000 | 0.0006 |
| 62 | 1,797 | 441,818 | 0.0015 | 0.0041 | 0.0000 | 0.0006 |
| 63 | 1,739 | 443,611 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 64 | 1,682 | 445,347 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 65 | 1,629 | 447,026 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 66 | 1,580 | 448,652 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 67 | 1,531 | 450,229 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 68 | 1,484 | 451,757 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 69 | 1,436 | 453,238 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 70 | 1,391 | 454,671 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 71 | 1,347 | 456,059 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 72 | 1,306 | 457,403 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 73 | 1,272 | 458,706 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 74 | 1,237 | 459,973 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 75 | 1,200 | 461,205 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 76 | 1,167 | 462,400 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 77 | 1,134 | 463,562 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 78 | 1,101 | 464,691 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 79 | 1,069 | 465,787 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 80 | 1,034 | 466,851 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 81 | 1,002 | 467,882 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 82 | 972 | 468,881 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 83 | 940 | 469,850 | 0.0016 | 0.0039 | 0.0000 | 0.0006 |
| 84 | 912 | 470,787 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 85 | 885 | 471,696 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 86 | 861 | 472,578 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 87 | 837 | 473,436 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 88 | 813 | 474,270 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 89 | 790 | 475,080 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 90 | 768 | 475,867 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 91 | 746 | 476,632 | 0.0014 | 0.0038 | 0.0000 | 0.0006 |
| 92 | 725 | 477,375 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 93 | 706 | 478,097 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 94 | 685 | 478,800 | 0.0024 | 0.0039 | 0.0000 | 0.0006 |
| 95 | 664 | 479,482 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 96 | 646 | 480,145 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 97 | 627 | 480,790 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 98 | 610 | 481,416 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 99 | 591 | 482,025 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 100 | 575 | 482,615 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 101 | 559 | 483,189 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 102 | 545 | 483,747 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 103 | 531 | 484,291 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 104 | 516 | 484,821 | 0.0015 | 0.0038 | 0.0000 | 0.0009 |
| 105 | 501 | 485,336 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 106 | 486 | 485,836 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 107 | 472 | 486,321 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 108 | 458 | 486,792 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 109 | 443 | 487,249 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 110 | 429 | 487,691 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 111 | 415 | 488,119 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 112 | 402 | 488,533 | 0.0014 | 0.0039 | 0.0000 | 0.0008 |
| 113 | 391 | 488,934 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 114 | 379 | 489,324 | 0.0016 | 0.0039 | 0.0000 | 0.0009 |
| 115 | 367 | 489,701 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 116 | 357 | 490,067 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 117 | 347 | 490,423 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 118 | 338 | 490,769 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 119 | 330 | 491,106 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 120 | 321 | 491,435 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 121 | 313 | 491,755 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 122 | 307 | 492,067 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 123 | 303 | 492,373 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 124 | 297 | 492,675 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 125 | 291 | 492,971 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 126 | 287 | 493,261 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 127 | 282 | 493,547 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 128 | 279 | 493,828 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 129 | 276 | 494,106 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 130 | 272 | 494,381 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 131 | 268 | 494,652 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 132 | 263 | 494,919 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 133 | 259 | 495,181 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 134 | 252 | 495,439 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 135 | 245 | 495,690 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 136 | 237 | 495,934 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 137 | 230 | 496,170 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 138 | 223 | 496,399 | 0.0017 | 0.0039 | 0.0000 | 0.0008 |
| 139 | 215 | 496,621 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 140 | 206 | 496,835 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 141 | 201 | 497,040 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 142 | 195 | 497,240 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 143 | 190 | 497,434 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 144 | 183 | 497,623 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 145 | 176 | 497,805 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 146 | 170 | 497,980 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 147 | 164 | 498,149 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 148 | 158 | 498,312 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 149 | 152 | 498,469 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 150 | 146 | 498,620 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 151 | 139 | 498,765 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 152 | 133 | 498,903 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 153 | 128 | 499,035 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 154 | 123 | 499,162 | 0.0015 | 0.0042 | 0.0000 | 0.0005 |
| 155 | 117 | 499,284 | 0.0016 | 0.0038 | 0.0000 | 0.0007 |
| 156 | 114 | 499,400 | 0.0015 | 0.0038 | 0.0000 | 0.0007 |
| 157 | 112 | 499,513 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 158 | 110 | 499,624 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 159 | 108 | 499,733 | 0.0015 | 0.0038 | 0.0000 | 0.0009 |
| 160 | 106 | 499,840 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 161 | 104 | 499,945 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 162 | 102 | 500,048 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 163 | 100 | 500,149 | 0.0014 | 0.0038 | 0.0000 | 0.0008 |
| 164 | 98 | 500,248 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 165 | 96 | 500,345 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 166 | 94 | 500,440 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 167 | 92 | 500,533 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 168 | 90 | 500,624 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 169 | 88 | 500,713 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 170 | 85 | 500,799 | 0.0015 | 0.0041 | 0.0000 | 0.0006 |
| 171 | 82 | 500,882 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 172 | 79 | 500,962 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 173 | 76 | 501,039 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 174 | 73 | 501,113 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 175 | 69 | 501,184 | 0.0015 | 0.0061 | 0.0000 | 0.0006 |
| 176 | 64 | 501,251 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 177 | 60 | 501,314 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 178 | 55 | 501,373 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 179 | 50 | 501,427 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 180 | 45 | 501,476 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 181 | 40 | 501,520 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 182 | 35 | 501,559 | 0.0015 | 0.0038 | 0.0000 | 0.0008 |
| 183 | 31 | 501,593 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 184 | 27 | 501,623 | 0.0014 | 0.0039 | 0.0000 | 0.0006 |
| 185 | 24 | 501,649 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 186 | 21 | 501,672 | 0.0015 | 0.0042 | 0.0000 | 0.0006 |
| 187 | 18 | 501,692 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 188 | 15 | 501,709 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 189 | 12 | 501,723 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 190 | 9 | 501,734 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 191 | 6 | 501,742 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 192 | 4 | 501,748 | 0.0015 | 0.0038 | 0.0000 | 0.0006 |
| 193 | 2 | 501,752 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 194 | 1 | 501,754 | 0.0015 | 0.0038 | 0.0000 | 0.0009 |
| 195 | 0 | 501,755 | 0.0015 | 0.0032 | 0.0000 | 0.0006 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| cal.cedge | 21,693 | 501,755 | 195 | 3,456 x 1,024 | 1.2287 |


Initialization: 0.0005, Read: 0.0045, reverse: 0.0003
Hashtable rate: 1,143,783,612 keys/s, time: 0.0000
Join: 0.3020
Projection: 0.0037
Deduplication: 0.7681
Memory clear: 0.0351
Union: 0.1145
Total: 1.2287

Benchmark for TG.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 22,471 | 23,874 | 0.0015 | 0.0032 | 0.0000 | 0.0000 |
| 2 | 22,375 | 45,838 | 0.0015 | 0.0032 | 0.0000 | 0.0001 |
| 3 | 23,637 | 66,624 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 4 | 25,782 | 87,014 | 0.0015 | 0.0033 | 0.0000 | 0.0003 |
| 5 | 28,560 | 107,321 | 0.0014 | 0.0031 | 0.0000 | 0.0000 |
| 6 | 31,840 | 127,667 | 0.0015 | 0.0035 | 0.0000 | 0.0000 |
| 7 | 35,460 | 148,007 | 0.0015 | 0.0035 | 0.0000 | 0.0000 |
| 8 | 39,337 | 168,222 | 0.0017 | 0.0042 | 0.0000 | 0.0004 |
| 9 | 43,371 | 188,149 | 0.0125 | 0.0079 | 0.0000 | 0.0004 |
| 10 | 47,414 | 207,747 | 0.0015 | 0.0034 | 0.0000 | 0.0003 |
| 11 | 51,351 | 226,937 | 0.0015 | 0.0035 | 0.0000 | 0.0004 |
| 12 | 55,127 | 245,569 | 0.0015 | 0.0036 | 0.0000 | 0.0005 |
| 13 | 58,670 | 263,578 | 0.0015 | 0.0037 | 0.0000 | 0.0009 |
| 14 | 62,029 | 280,898 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 15 | 65,016 | 297,495 | 0.0015 | 0.0038 | 0.0000 | 0.0005 |
| 16 | 67,792 | 313,305 | 0.0015 | 0.0038 | 0.0001 | 0.0005 |
| 17 | 70,145 | 328,407 | 0.0015 | 0.0039 | 0.0001 | 0.0005 |
| 18 | 71,909 | 342,636 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 19 | 73,064 | 355,917 | 0.0015 | 0.0038 | 0.0001 | 0.0005 |
| 20 | 73,683 | 368,221 | 0.0015 | 0.0057 | 0.0001 | 0.0004 |
| 21 | 73,821 | 379,601 | 0.0015 | 0.0039 | 0.0000 | 0.0005 |
| 22 | 73,437 | 390,144 | 0.0016 | 0.0040 | 0.0001 | 0.0006 |
| 23 | 72,723 | 399,786 | 0.0015 | 0.0040 | 0.0001 | 0.0005 |
| 24 | 71,591 | 408,600 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 25 | 70,144 | 416,622 | 0.0017 | 0.0040 | 0.0001 | 0.0005 |
| 26 | 68,467 | 423,911 | 0.0015 | 0.0039 | 0.0001 | 0.0005 |
| 27 | 66,570 | 430,545 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 28 | 64,480 | 436,571 | 0.0015 | 0.0039 | 0.0000 | 0.0008 |
| 29 | 62,259 | 442,041 | 0.0015 | 0.0039 | 0.0002 | 0.0006 |
| 30 | 59,796 | 446,999 | 0.0015 | 0.0039 | 0.0001 | 0.0006 |
| 31 | 57,243 | 451,424 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 32 | 54,559 | 455,377 | 0.0015 | 0.0039 | 0.0000 | 0.0009 |
| 33 | 51,796 | 458,896 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 34 | 49,012 | 461,978 | 0.0015 | 0.0041 | 0.0000 | 0.0006 |
| 35 | 46,217 | 464,659 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 36 | 43,494 | 467,015 | 0.0015 | 0.0040 | 0.0000 | 0.0007 |
| 37 | 40,877 | 469,123 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 38 | 38,399 | 471,008 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 39 | 35,988 | 472,687 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 40 | 33,679 | 474,152 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 41 | 31,442 | 475,408 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 42 | 29,250 | 476,476 | 0.0015 | 0.0040 | 0.0001 | 0.0005 |
| 43 | 27,107 | 477,382 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 44 | 25,048 | 478,152 | 0.0015 | 0.0040 | 0.0000 | 0.0007 |
| 45 | 23,074 | 478,795 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 46 | 21,168 | 479,331 | 0.0016 | 0.0039 | 0.0000 | 0.0007 |
| 47 | 19,362 | 479,761 | 0.0015 | 0.0040 | 0.0000 | 0.0009 |
| 48 | 17,640 | 480,103 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 49 | 16,033 | 480,376 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 50 | 14,527 | 480,597 | 0.0015 | 0.0040 | 0.0000 | 0.0006 |
| 51 | 13,113 | 480,774 | 0.0015 | 0.0039 | 0.0000 | 0.0005 |
| 52 | 11,765 | 480,906 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 53 | 10,504 | 480,998 | 0.0015 | 0.0039 | 0.0000 | 0.0007 |
| 54 | 9,324 | 481,059 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 55 | 8,247 | 481,096 | 0.0015 | 0.0042 | 0.0000 | 0.0009 |
| 56 | 7,270 | 481,114 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 57 | 6,366 | 481,120 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |
| 58 | 5,573 | 481,121 | 0.0015 | 0.0039 | 0.0000 | 0.0006 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| TG.cedge | 23,874 | 481,121 | 58 | 3,456 x 1,024 | 0.3733 |


Initialization: 0.0003, Read: 0.0048, reverse: 0.0002
Hashtable rate: 1,193,222,710 keys/s, time: 0.0000
Join: 0.0972
Projection: 0.0022
Deduplication: 0.2283
Memory clear: 0.0098
Union: 0.0305
Total: 0.3733

Benchmark for OL.cedge
----------------------------------------------------------
| Iteration | # Deduplicated join | # Deduplicated union | Join(s) | Deduplication(s) | Projection(s) | Union(s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 7,331 | 7,035 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 2 | 7,628 | 14,319 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 3 | 7,848 | 21,813 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 4 | 8,017 | 29,352 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 5 | 8,134 | 36,851 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 6 | 8,214 | 44,235 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 7 | 8,263 | 51,441 | 0.0015 | 0.0030 | 0.0000 | 0.0002 |
| 8 | 8,293 | 58,424 | 0.0015 | 0.0030 | 0.0000 | 0.0004 |
| 9 | 8,321 | 65,147 | 0.0015 | 0.0031 | 0.0000 | 0.0003 |
| 10 | 8,317 | 71,596 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 11 | 8,357 | 77,694 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 12 | 8,415 | 83,457 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 13 | 8,448 | 88,894 | 0.0015 | 0.0032 | 0.0000 | 0.0003 |
| 14 | 8,467 | 93,984 | 0.0015 | 0.0034 | 0.0000 | 0.0000 |
| 15 | 8,463 | 98,738 | 0.0015 | 0.0032 | 0.0000 | 0.0005 |
| 16 | 8,424 | 103,161 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 17 | 8,352 | 107,252 | 0.0015 | 0.0032 | 0.0000 | 0.0005 |
| 18 | 8,248 | 111,015 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 19 | 8,133 | 114,460 | 0.0015 | 0.0032 | 0.0000 | 0.0005 |
| 20 | 7,979 | 117,619 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 21 | 7,754 | 120,475 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 22 | 7,541 | 123,011 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 23 | 7,341 | 125,308 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 24 | 7,127 | 127,436 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 25 | 6,877 | 129,399 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 26 | 6,591 | 131,199 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 27 | 6,259 | 132,828 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 28 | 5,883 | 134,275 | 0.0014 | 0.0031 | 0.0000 | 0.0000 |
| 29 | 5,469 | 135,536 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 30 | 5,038 | 136,633 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 31 | 4,611 | 137,589 | 0.0014 | 0.0031 | 0.0000 | 0.0000 |
| 32 | 4,192 | 138,429 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 33 | 3,797 | 139,178 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 34 | 3,427 | 139,847 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 35 | 3,071 | 140,452 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 36 | 2,726 | 140,995 | 0.0017 | 0.0031 | 0.0000 | 0.0000 |
| 37 | 2,414 | 141,481 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 38 | 2,134 | 141,924 | 0.0015 | 0.0031 | 0.0000 | 0.0000 |
| 39 | 1,887 | 142,327 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 40 | 1,665 | 142,694 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 41 | 1,467 | 143,034 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 42 | 1,280 | 143,360 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 43 | 1,109 | 143,675 | 0.0016 | 0.0030 | 0.0000 | 0.0000 |
| 44 | 960 | 143,976 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 45 | 824 | 144,264 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 46 | 709 | 144,532 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 47 | 608 | 144,779 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 48 | 523 | 145,002 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 49 | 445 | 145,198 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 50 | 379 | 145,369 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 51 | 322 | 145,518 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 52 | 272 | 145,645 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 53 | 227 | 145,755 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 54 | 190 | 145,847 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 55 | 158 | 145,922 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 56 | 130 | 145,980 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 57 | 105 | 146,024 | 0.0015 | 0.0032 | 0.0000 | 0.0000 |
| 58 | 81 | 146,057 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 59 | 61 | 146,082 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 60 | 44 | 146,098 | 0.0014 | 0.0030 | 0.0000 | 0.0000 |
| 61 | 34 | 146,109 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 62 | 24 | 146,116 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 63 | 17 | 146,119 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |
| 64 | 12 | 146,120 | 0.0015 | 0.0030 | 0.0000 | 0.0000 |

| Dataset | Number of rows | TC size | Iterations | Blocks x Threads | Time (s) |
| --- | --- | --- | --- | --- | --- |
| OL.cedge | 7,035 | 146,120 | 64 | 3,456 x 1,024 | 0.3000 |


Initialization: 0.0003, Read: 0.0016, reverse: 0.0001
Hashtable rate: 353,215,845 keys/s, time: 0.0000
Join: 0.0941
Projection: 0.0012
Deduplication: 0.1962
Memory clear: 0.0020
Union: 0.0045
Total: 0.3000

## Rapids Intermediate Result
```
python transitive_closure_intermediate.py
```

Benchmark for SF.cedge
----------------------------------------------------------
| Iteration | # Deduplicated union | Join(s) | Deduplication+Projection+Union(s) |
| --- | --- | --- | --- |
| 2 | 0.0049 | 476246 |0.0193 |
| 3 | 0.0059 | 755982 |0.0204 |
| 4 | 0.0059 | 1056726 |0.0212 |
| 5 | 0.0059 | 1376010 |0.0213 |
| 6 | 0.0059 | 1711750 |0.0218 |
| 7 | 0.0061 | 2062150 |0.0227 |
| 8 | 0.0067 | 2426401 |0.0234 |
| 9 | 0.0073 | 2803627 |0.0239 |
| 10 | 0.0076 | 3193373 |0.0349 |
| 11 | 0.0086 | 3595385 |0.0259 |
| 12 | 0.0094 | 4009951 |0.0267 |
| 13 | 0.0117 | 4437031 |0.0277 |
| 14 | 0.0104 | 4876068 |0.0283 |
| 15 | 0.0104 | 5326697 |0.0320 |
| 16 | 0.0104 | 5788461 |0.0325 |
| 17 | 0.0104 | 6260652 |0.0383 |
| 18 | 0.0112 | 6742695 |0.0366 |
| 19 | 0.0103 | 7233796 |0.0375 |
| 20 | 0.0127 | 7733688 |0.0386 |
| 21 | 0.0137 | 8241532 |0.0401 |
| 22 | 0.0137 | 8756848 |0.0416 |
| 23 | 0.0136 | 9279010 |0.0577 |
| 24 | 0.0147 | 9807603 |0.0562 |
| 25 | 0.0147 | 10341798 |0.0508 |
| 26 | 0.0146 | 10881222 |0.0497 |
| 27 | 0.0146 | 11425295 |0.0507 |
| 28 | 0.0156 | 11973638 |0.0537 |
| 29 | 0.0156 | 12526796 |0.0545 |
| 30 | 0.0146 | 13084624 |0.0582 |
| 31 | 0.0158 | 13646848 |0.0573 |
| 32 | 0.0219 | 14212245 |0.0604 |
| 33 | 0.0155 | 14779678 |0.0614 |
| 34 | 0.0164 | 15348483 |0.0765 |
| 35 | 0.0154 | 15918265 |0.0653 |
| 36 | 0.0165 | 16488145 |0.0663 |
| 37 | 0.0166 | 17057403 |0.0676 |
| 38 | 0.0167 | 17625807 |0.0695 |
| 39 | 0.0167 | 18193301 |0.0717 |
| 40 | 0.0190 | 18759980 |0.0694 |
| 41 | 0.0299 | 19325886 |0.0712 |
| 42 | 0.0189 | 19890746 |0.0736 |
| 43 | 0.0200 | 20454047 |0.0747 |
| 44 | 0.0200 | 21015866 |0.0757 |
| 45 | 0.0200 | 21576436 |0.0778 |
| 46 | 0.0198 | 22135333 |0.0780 |
| 47 | 0.0198 | 22693109 |0.0790 |
| 48 | 0.0312 | 23249591 |0.0814 |
| 49 | 0.0218 | 23804594 |0.0833 |
| 50 | 0.0218 | 24357862 |0.0877 |
| 51 | 0.0215 | 24909212 |0.0856 |
| 52 | 0.0217 | 25458554 |0.0901 |
| 53 | 0.0211 | 26006351 |0.0887 |
| 54 | 0.0219 | 26552384 |0.0929 |
| 55 | 0.0216 | 27096892 |0.0920 |
| 56 | 0.0215 | 27639482 |0.0931 |
| 57 | 0.0216 | 28179796 |0.0962 |
| 58 | 0.0215 | 28717697 |0.0962 |
| 59 | 0.0214 | 29253290 |0.0975 |
| 60 | 0.0215 | 29786407 |0.1006 |
| 61 | 0.0215 | 30316892 |0.1004 |
| 62 | 0.0214 | 30844793 |0.1019 |
| 63 | 0.0211 | 31369749 |0.1029 |
| 64 | 0.0222 | 31891466 |0.1102 |
| 65 | 0.0325 | 32409890 |0.1080 |
| 66 | 0.0235 | 32925490 |0.1093 |
| 67 | 0.0243 | 33437622 |0.1100 |
| 68 | 0.0232 | 33946228 |0.1093 |
| 69 | 0.0263 | 34451545 |0.1136 |
| 70 | 0.0241 | 34953666 |0.1271 |
| 71 | 0.0241 | 35452811 |0.1633 |
| 72 | 0.0236 | 35949111 |0.1694 |
| 73 | 0.0237 | 36442445 |0.1739 |
| 74 | 0.0234 | 36932782 |0.1998 |
| 75 | 0.0239 | 37420101 |0.1937 |
| 76 | 0.0236 | 37904568 |0.2133 |
| 77 | 0.0240 | 38386437 |0.1846 |
| 78 | 0.0242 | 38865648 |0.2120 |
| 79 | 0.0252 | 39341838 |0.2068 |
| 80 | 0.0244 | 39814740 |0.2002 |
| 81 | 0.0243 | 40284305 |0.1906 |
| 82 | 0.0236 | 40750481 |0.2282 |
| 83 | 0.0241 | 41213907 |0.2296 |
| 84 | 0.0243 | 41674629 |0.2527 |
| 85 | 0.0348 | 42132970 |0.2162 |
| 86 | 0.0253 | 42589055 |0.1705 |
| 87 | 0.0236 | 43042721 |0.2227 |
| 88 | 0.0249 | 43493531 |0.2781 |
| 89 | 0.0255 | 43941165 |0.1679 |
| 90 | 0.0263 | 44385331 |0.2882 |
| 91 | 0.0252 | 44826120 |0.1797 |
| 92 | 0.0250 | 45263361 |0.3003 |
| 93 | 0.0261 | 45697225 |0.1819 |
| 94 | 0.0281 | 46127985 |0.2599 |
| 95 | 0.0323 | 46556237 |0.2452 |
| 96 | 0.0270 | 46981573 |0.2397 |
| 97 | 0.0257 | 47403868 |0.2327 |
| 98 | 0.0262 | 47822709 |0.2274 |
| 99 | 0.0267 | 48238000 |0.2288 |
| 100 | 0.0345 | 48649760 |0.3070 |
| 101 | 0.0258 | 49058284 |0.2695 |
| 102 | 0.0295 | 49463876 |0.1991 |
| 103 | 0.0263 | 49866997 |0.2410 |
| 104 | 0.0264 | 50267588 |0.2437 |
| 105 | 0.0261 | 50665511 |0.2580 |
| 106 | 0.0266 | 51061106 |0.2780 |
| 107 | 0.0264 | 51454139 |0.3333 |
| 108 | 0.0266 | 51844140 |0.2322 |
| 109 | 0.0283 | 52231327 |0.2219 |
| 110 | 0.0266 | 52615475 |0.2787 |
| 111 | 0.0266 | 52996813 |0.4090 |
| 112 | 0.0270 | 53375817 |0.1996 |
| 113 | 0.0268 | 53752812 |0.2533 |
| 114 | 0.0270 | 54127794 |0.2944 |
| 115 | 0.0280 | 54500796 |0.3425 |
| 116 | 0.0273 | 54872132 |0.2246 |
| 117 | 0.0269 | 55241914 |0.3307 |
| 118 | 0.0283 | 55610441 |0.2540 |
| 119 | 0.0300 | 55977545 |0.3107 |
| 120 | 0.0367 | 56343282 |0.2698 |
| 121 | 0.0408 | 56707575 |0.2972 |
| 122 | 0.0368 | 57070595 |0.2664 |
| 123 | 0.0411 | 57431978 |0.3058 |
| 124 | 0.0370 | 57791689 |0.2885 |
| 125 | 0.0328 | 58150108 |0.3221 |
| 126 | 0.0286 | 58507184 |0.2094 |
| 127 | 0.0292 | 58863245 |0.4141 |
| 128 | 0.0286 | 59218664 |0.2440 |
| 129 | 0.0298 | 59573836 |0.3334 |
| 130 | 0.0285 | 59929167 |0.2235 |
| 131 | 0.0283 | 60284508 |0.3335 |
| 132 | 0.0297 | 60639440 |0.3679 |
| 133 | 0.0297 | 60994007 |0.2768 |
| 134 | 0.0299 | 61348377 |0.3112 |
| 135 | 0.0283 | 61702405 |0.3639 |
| 136 | 0.0283 | 62055865 |0.2738 |
| 137 | 0.0296 | 62408260 |0.2900 |
| 138 | 0.0295 | 62759153 |0.3674 |
| 139 | 0.0299 | 63107949 |0.2986 |
| 140 | 0.0299 | 63454403 |0.3226 |
| 141 | 0.0300 | 63798667 |0.3162 |
| 142 | 0.0300 | 64141120 |0.3270 |
| 143 | 0.0311 | 64482480 |0.3235 |
| 144 | 0.0312 | 64822707 |0.3361 |
| 145 | 0.0313 | 65161915 |0.3309 |
| 146 | 0.0301 | 65499978 |0.3298 |
| 147 | 0.0302 | 65836150 |0.3399 |
| 148 | 0.0305 | 66170073 |0.3794 |
| 149 | 0.0307 | 66502122 |0.3584 |
| 150 | 0.0309 | 66832236 |0.2985 |
| 151 | 0.0365 | 67159849 |0.3619 |
| 152 | 0.0332 | 67485035 |0.2983 |
| 153 | 0.0313 | 67806862 |0.2978 |
| 154 | 0.0306 | 68124852 |0.4071 |
| 155 | 0.0296 | 68438764 |0.3468 |
| 156 | 0.0301 | 68748701 |0.4110 |
| 157 | 0.0307 | 69054594 |0.3098 |
| 158 | 0.0324 | 69356883 |0.3543 |
| 159 | 0.0308 | 69655712 |0.4124 |
| 160 | 0.0322 | 69950812 |0.3057 |
| 161 | 0.0335 | 70241914 |0.3381 |
| 162 | 0.0325 | 70528877 |0.3186 |
| 163 | 0.0320 | 70811829 |0.4166 |
| 164 | 0.0303 | 71090923 |0.3408 |
| 165 | 0.0328 | 71366182 |0.4729 |
| 166 | 0.0320 | 71637611 |0.3191 |
| 167 | 0.0303 | 71905185 |0.3395 |
| 168 | 0.0321 | 72168945 |0.3229 |
| 169 | 0.0360 | 72428653 |0.4205 |
| 170 | 0.0328 | 72684006 |0.3785 |
| 171 | 0.0430 | 72934972 |0.3873 |
| 172 | 0.0468 | 73181708 |0.3499 |
| 173 | 0.0319 | 73424676 |0.3435 |
| 174 | 0.0321 | 73664310 |0.4233 |
| 175 | 0.0332 | 73900521 |0.3300 |
| 176 | 0.0341 | 74133461 |0.3948 |
| 177 | 0.0407 | 74363061 |0.3300 |
| 178 | 0.0326 | 74588756 |0.3693 |
| 179 | 0.0330 | 74810673 |0.3587 |
| 180 | 0.0336 | 75027937 |0.4773 |
| 181 | 0.0322 | 75240140 |0.3238 |
| 182 | 0.0315 | 75446962 |0.4016 |
| 183 | 0.0317 | 75648307 |0.4051 |
| 184 | 0.0315 | 75844252 |0.4031 |
| 185 | 0.0334 | 76034789 |0.3410 |
| 186 | 0.0315 | 76219965 |0.4078 |
| 187 | 0.0329 | 76399917 |0.3872 |
| 188 | 0.0325 | 76574662 |0.3217 |
| 189 | 0.0332 | 76744231 |0.3520 |
| 190 | 0.0320 | 76908857 |0.4274 |
| 191 | 0.0321 | 77068533 |0.4152 |
| 192 | 0.0329 | 77223086 |0.3942 |
| 193 | 0.0320 | 77372718 |0.3886 |
| 194 | 0.0348 | 77517227 |0.2938 |
| 195 | 0.0325 | 77656631 |0.4652 |
| 196 | 0.0323 | 77791065 |0.3809 |
| 197 | 0.0324 | 77920428 |0.3635 |
| 198 | 0.0370 | 78044726 |0.4207 |
| 199 | 0.0315 | 78164252 |0.4068 |
| 200 | 0.0311 | 78279222 |0.2881 |
| 201 | 0.0359 | 78389839 |0.4545 |
| 202 | 0.0314 | 78496604 |0.3182 |
| 203 | 0.0308 | 78599703 |0.4057 |
| 204 | 0.0510 | 78699354 |0.3174 |
| 205 | 0.0335 | 78795763 |0.3201 |
| 206 | 0.0363 | 78888736 |0.3286 |
| 207 | 0.0306 | 78978233 |0.4614 |
| 208 | 0.0305 | 79064276 |0.3901 |
| 209 | 0.0328 | 79146714 |0.3519 |
| 210 | 0.0300 | 79225481 |0.4485 |
| 211 | 0.0313 | 79300584 |0.3500 |
| 212 | 0.0296 | 79372082 |0.3131 |
| 213 | 0.0312 | 79440053 |0.4664 |
| 214 | 0.0310 | 79504504 |0.2949 |
| 215 | 0.0305 | 79565625 |0.2995 |
| 216 | 0.0362 | 79623487 |0.3918 |
| 217 | 0.0298 | 79678253 |0.3267 |
| 218 | 0.0303 | 79730071 |0.4272 |
| 219 | 0.0305 | 79779016 |0.4343 |
| 220 | 0.0298 | 79825181 |0.3524 |
| 221 | 0.0303 | 79868670 |0.3108 |
| 222 | 0.0295 | 79909515 |0.4151 |
| 223 | 0.0292 | 79947770 |0.3814 |
| 224 | 0.0293 | 79983633 |0.3222 |
| 225 | 0.0304 | 80017213 |0.4640 |
| 226 | 0.0282 | 80048760 |0.2815 |
| 227 | 0.0301 | 80078463 |0.3335 |
| 228 | 0.0336 | 80106504 |0.4377 |
| 229 | 0.0291 | 80133134 |0.3107 |
| 230 | 0.0294 | 80158393 |0.3590 |
| 231 | 0.0285 | 80182374 |0.3634 |
| 232 | 0.0291 | 80205090 |0.2855 |
| 233 | 0.0282 | 80226530 |0.3900 |
| 234 | 0.0285 | 80246765 |0.3810 |
| 235 | 0.0287 | 80265670 |0.2920 |
| 236 | 0.0280 | 80283284 |0.3556 |
| 237 | 0.0284 | 80299769 |0.4596 |
| 238 | 0.0296 | 80315319 |0.2945 |
| 239 | 0.0292 | 80330019 |0.3665 |
| 240 | 0.0289 | 80343919 |0.3139 |
| 241 | 0.0281 | 80357024 |0.3947 |
| 242 | 0.0284 | 80369267 |0.3330 |
| 243 | 0.0269 | 80380641 |0.3806 |
| 244 | 0.0277 | 80391114 |0.2819 |
| 245 | 0.0278 | 80400766 |0.3388 |
| 246 | 0.0286 | 80409684 |0.3504 |
| 247 | 0.0601 | 80417910 |0.2654 |
| 248 | 0.0262 | 80425562 |0.3738 |
| 249 | 0.0258 | 80432654 |0.3474 |
| 250 | 0.0274 | 80439173 |0.3388 |
| 251 | 0.0264 | 80445155 |0.3718 |
| 252 | 0.0263 | 80450578 |0.3368 |
| 253 | 0.0263 | 80455469 |0.3442 |
| 254 | 0.0265 | 80459897 |0.3316 |
| 255 | 0.0274 | 80463897 |0.3264 |
| 256 | 0.0263 | 80467539 |0.3485 |
| 257 | 0.0555 | 80470869 |0.3287 |
| 258 | 0.0293 | 80473927 |0.3199 |
| 259 | 0.0263 | 80476772 |0.2928 |
| 260 | 0.0265 | 80479391 |0.3570 |
| 261 | 0.0259 | 80481772 |0.2998 |
| 262 | 0.0266 | 80483924 |0.3325 |
| 263 | 0.0262 | 80485843 |0.3040 |
| 264 | 0.0262 | 80487531 |0.3475 |
| 265 | 0.0257 | 80489028 |0.3094 |
| 266 | 0.0265 | 80490346 |0.3516 |
| 267 | 0.0261 | 80491511 |0.3368 |
| 268 | 0.0265 | 80492545 |0.3085 |
| 269 | 0.0264 | 80493456 |0.2748 |
| 270 | 0.0268 | 80494260 |0.3621 |
| 271 | 0.0259 | 80494968 |0.2871 |
| 272 | 0.0246 | 80495572 |0.3140 |
| 273 | 0.0242 | 80496081 |0.3177 |
| 274 | 0.0246 | 80496493 |0.2620 |
| 275 | 0.0241 | 80496825 |0.4118 |
| 276 | 0.0249 | 80497095 |0.2475 |
| 277 | 0.0233 | 80497309 |0.3147 |
| 278 | 0.0234 | 80497473 |0.2940 |
| 279 | 0.0252 | 80497614 |0.3587 |
| 280 | 0.0245 | 80497730 |0.2786 |
| 281 | 0.0234 | 80497823 |0.3614 |
| 282 | 0.0240 | 80497895 |0.2641 |
| 283 | 0.0236 | 80497948 |0.2941 |
| 284 | 0.0241 | 80497984 |0.3163 |
| 285 | 0.0239 | 80498003 |0.3775 |
| 286 | 0.0237 | 80498011 |0.2606 |
| 287 | 0.0244 | 80498014 |0.2811 |

Read: 0.0384, reverse: 0.0395
Join: 74.5477
Projection, deduplication, union: 74.5477
Memory clear: 0.4167
Total: 82.6554

| Dataset | Number of rows | TC size | Iterations | Time (s) |
| --- | --- | --- | --- | --- |
| SF.cedge | 223001 | 80498014 | 287 | 82.6554 |

Benchmark for cal.cedge
----------------------------------------------------------
| Iteration | # Deduplicated union | Join(s) | Deduplication+Projection+Union(s) |
| --- | --- | --- | --- |
| 2 | 0.0012 | 41526 |0.0049 |
| 3 | 0.0010 | 60132 |0.0041 |
| 4 | 0.0013 | 77715 |0.0041 |
| 5 | 0.0016 | 94383 |0.0048 |
| 6 | 0.0012 | 110206 |0.0052 |
| 7 | 0.0013 | 125261 |0.0055 |
| 8 | 0.0013 | 139584 |0.0079 |
| 9 | 0.0013 | 153218 |0.0078 |
| 10 | 0.0009 | 166192 |0.0071 |
| 11 | 0.0016 | 178538 |0.0085 |
| 12 | 0.0009 | 190304 |0.0087 |
| 13 | 0.0016 | 201538 |0.0089 |
| 14 | 0.0013 | 212277 |0.0086 |
| 15 | 0.0009 | 222554 |0.0082 |
| 16 | 0.0016 | 232384 |0.0079 |
| 17 | 0.0013 | 241797 |0.0075 |
| 18 | 0.0019 | 250810 |0.0089 |
| 19 | 0.0016 | 259449 |0.0103 |
| 20 | 0.0016 | 267745 |0.0116 |
| 21 | 0.0009 | 275713 |0.0264 |
| 22 | 0.0009 | 283363 |0.0116 |
| 23 | 0.0009 | 290703 |0.0116 |
| 24 | 0.0009 | 297762 |0.0116 |
| 25 | 0.0009 | 304561 |0.0115 |
| 26 | 0.0009 | 311114 |0.0115 |
| 27 | 0.0009 | 317434 |0.0116 |
| 28 | 0.0009 | 323532 |0.0116 |
| 29 | 0.0009 | 329409 |0.0116 |
| 30 | 0.0009 | 335075 |0.0117 |
| 31 | 0.0009 | 340538 |0.0116 |
| 32 | 0.0009 | 345808 |0.0116 |
| 33 | 0.0009 | 350899 |0.0157 |
| 34 | 0.0010 | 355820 |0.0116 |
| 35 | 0.0009 | 360571 |0.0114 |
| 36 | 0.0009 | 365155 |0.0118 |
| 37 | 0.0009 | 369583 |0.0116 |
| 38 | 0.0009 | 373855 |0.0138 |
| 39 | 0.0009 | 377981 |0.0115 |
| 40 | 0.0009 | 381964 |0.0116 |
| 41 | 0.0009 | 385813 |0.0116 |
| 42 | 0.0009 | 389531 |0.0115 |
| 43 | 0.0009 | 393123 |0.0117 |
| 44 | 0.0009 | 396594 |0.0115 |
| 45 | 0.0009 | 399943 |0.0115 |
| 46 | 0.0009 | 403163 |0.0117 |
| 47 | 0.0009 | 406263 |0.0115 |
| 48 | 0.0009 | 409243 |0.0115 |
| 49 | 0.0009 | 412116 |0.0116 |
| 50 | 0.0009 | 414893 |0.0116 |
| 51 | 0.0009 | 417579 |0.0118 |
| 52 | 0.0009 | 420175 |0.0116 |
| 53 | 0.0009 | 422685 |0.0115 |
| 54 | 0.0009 | 425112 |0.0115 |
| 55 | 0.0009 | 427454 |0.0115 |
| 56 | 0.0009 | 429715 |0.0115 |
| 57 | 0.0009 | 431902 |0.0115 |
| 58 | 0.0009 | 434019 |0.0115 |
| 59 | 0.0009 | 436068 |0.0117 |
| 60 | 0.0009 | 438049 |0.0114 |
| 61 | 0.0009 | 439966 |0.0115 |
| 62 | 0.0009 | 441818 |0.0115 |
| 63 | 0.0009 | 443611 |0.0116 |
| 64 | 0.0009 | 445347 |0.0115 |
| 65 | 0.0009 | 447026 |0.0115 |
| 66 | 0.0009 | 448652 |0.0215 |
| 67 | 0.0009 | 450229 |0.0115 |
| 68 | 0.0009 | 451757 |0.0116 |
| 69 | 0.0009 | 453238 |0.0115 |
| 70 | 0.0009 | 454671 |0.0134 |
| 71 | 0.0009 | 456059 |0.0115 |
| 72 | 0.0009 | 457403 |0.0115 |
| 73 | 0.0009 | 458706 |0.0114 |
| 74 | 0.0009 | 459973 |0.0115 |
| 75 | 0.0009 | 461205 |0.0114 |
| 76 | 0.0009 | 462400 |0.0114 |
| 77 | 0.0009 | 463562 |0.0114 |
| 78 | 0.0009 | 464691 |0.0133 |
| 79 | 0.0009 | 465787 |0.0114 |
| 80 | 0.0009 | 466851 |0.0115 |
| 81 | 0.0009 | 467882 |0.0115 |
| 82 | 0.0009 | 468881 |0.0117 |
| 83 | 0.0009 | 469850 |0.0117 |
| 84 | 0.0009 | 470787 |0.0115 |
| 85 | 0.0009 | 471696 |0.0116 |
| 86 | 0.0009 | 472578 |0.0116 |
| 87 | 0.0009 | 473436 |0.0116 |
| 88 | 0.0009 | 474270 |0.0117 |
| 89 | 0.0009 | 475080 |0.0115 |
| 90 | 0.0009 | 475867 |0.0118 |
| 91 | 0.0009 | 476632 |0.0115 |
| 92 | 0.0009 | 477375 |0.0116 |
| 93 | 0.0009 | 478097 |0.0115 |
| 94 | 0.0009 | 478800 |0.0116 |
| 95 | 0.0009 | 479482 |0.0116 |
| 96 | 0.0009 | 480145 |0.0115 |
| 97 | 0.0009 | 480790 |0.0115 |
| 98 | 0.0009 | 481416 |0.0117 |
| 99 | 0.0009 | 482025 |0.0116 |
| 100 | 0.0009 | 482615 |0.0116 |
| 101 | 0.0009 | 483189 |0.0116 |
| 102 | 0.0009 | 483747 |0.0115 |
| 103 | 0.0009 | 484291 |0.0115 |
| 104 | 0.0009 | 484821 |0.0116 |
| 105 | 0.0009 | 485336 |0.0117 |
| 106 | 0.0009 | 485836 |0.0118 |
| 107 | 0.0009 | 486321 |0.0116 |
| 108 | 0.0009 | 486792 |0.0116 |
| 109 | 0.0009 | 487249 |0.0116 |
| 110 | 0.0009 | 487691 |0.0116 |
| 111 | 0.0009 | 488119 |0.0115 |
| 112 | 0.0009 | 488533 |0.0116 |
| 113 | 0.0008 | 488934 |0.0115 |
| 114 | 0.0009 | 489324 |0.0187 |
| 115 | 0.0009 | 489701 |0.0178 |
| 116 | 0.0009 | 490067 |0.0115 |
| 117 | 0.0009 | 490423 |0.0116 |
| 118 | 0.0009 | 490769 |0.0217 |
| 119 | 0.0009 | 491106 |0.0115 |
| 120 | 0.0009 | 491435 |0.0117 |
| 121 | 0.0009 | 491755 |0.0116 |
| 122 | 0.0009 | 492067 |0.0171 |
| 123 | 0.0009 | 492373 |0.0116 |
| 124 | 0.0009 | 492675 |0.0115 |
| 125 | 0.0009 | 492971 |0.0115 |
| 126 | 0.0009 | 493261 |0.0116 |
| 127 | 0.0009 | 493547 |0.0115 |
| 128 | 0.0009 | 493828 |0.0117 |
| 129 | 0.0009 | 494106 |0.0115 |
| 130 | 0.0009 | 494381 |0.0115 |
| 131 | 0.0009 | 494652 |0.0115 |
| 132 | 0.0009 | 494919 |0.0115 |
| 133 | 0.0009 | 495181 |0.0116 |
| 134 | 0.0009 | 495439 |0.0116 |
| 135 | 0.0009 | 495690 |0.0116 |
| 136 | 0.0009 | 495934 |0.0155 |
| 137 | 0.0009 | 496170 |0.0116 |
| 138 | 0.0009 | 496399 |0.0116 |
| 139 | 0.0009 | 496621 |0.0116 |
| 140 | 0.0009 | 496835 |0.0115 |
| 141 | 0.0009 | 497040 |0.0116 |
| 142 | 0.0009 | 497240 |0.0117 |
| 143 | 0.0009 | 497434 |0.0119 |
| 144 | 0.0008 | 497623 |0.0116 |
| 145 | 0.0009 | 497805 |0.0116 |
| 146 | 0.0009 | 497980 |0.0116 |
| 147 | 0.0008 | 498149 |0.0115 |
| 148 | 0.0009 | 498312 |0.0137 |
| 149 | 0.0009 | 498469 |0.0115 |
| 150 | 0.0009 | 498620 |0.0114 |
| 151 | 0.0009 | 498765 |0.0117 |
| 152 | 0.0009 | 498903 |0.0116 |
| 153 | 0.0009 | 499035 |0.0115 |
| 154 | 0.0008 | 499162 |0.0115 |
| 155 | 0.0008 | 499284 |0.0115 |
| 156 | 0.0008 | 499400 |0.0115 |
| 157 | 0.0009 | 499513 |0.0114 |
| 158 | 0.0009 | 499624 |0.0174 |
| 159 | 0.0008 | 499733 |0.0118 |
| 160 | 0.0008 | 499840 |0.0115 |
| 161 | 0.0009 | 499945 |0.0115 |
| 162 | 0.0009 | 500048 |0.0115 |
| 163 | 0.0009 | 500149 |0.0117 |
| 164 | 0.0009 | 500248 |0.0115 |
| 165 | 0.0009 | 500345 |0.0116 |
| 166 | 0.0009 | 500440 |0.0115 |
| 167 | 0.0009 | 500533 |0.0115 |
| 168 | 0.0008 | 500624 |0.0116 |
| 169 | 0.0009 | 500713 |0.0114 |
| 170 | 0.0009 | 500799 |0.0117 |
| 171 | 0.0009 | 500882 |0.0114 |
| 172 | 0.0009 | 500962 |0.0115 |
| 173 | 0.0009 | 501039 |0.0114 |
| 174 | 0.0009 | 501113 |0.0242 |
| 175 | 0.0009 | 501184 |0.0114 |
| 176 | 0.0009 | 501251 |0.0115 |
| 177 | 0.0009 | 501314 |0.0116 |
| 178 | 0.0009 | 501373 |0.0115 |
| 179 | 0.0009 | 501427 |0.0115 |
| 180 | 0.0009 | 501476 |0.0115 |
| 181 | 0.0008 | 501520 |0.0115 |
| 182 | 0.0009 | 501559 |0.0115 |
| 183 | 0.0008 | 501593 |0.0115 |
| 184 | 0.0008 | 501623 |0.0115 |
| 185 | 0.0009 | 501649 |0.0114 |
| 186 | 0.0009 | 501672 |0.0115 |
| 187 | 0.0009 | 501692 |0.0114 |
| 188 | 0.0009 | 501709 |0.0115 |
| 189 | 0.0008 | 501723 |0.0114 |
| 190 | 0.0008 | 501734 |0.0117 |
| 191 | 0.0009 | 501742 |0.0117 |
| 192 | 0.0009 | 501748 |0.0113 |
| 193 | 0.0009 | 501752 |0.0113 |
| 194 | 0.0008 | 501754 |0.0113 |
| 195 | 0.0008 | 501755 |0.0131 |

Read: 1.3486, reverse: 1.3575
Join: 2.2626
Projection, deduplication, union: 2.2626
Memory clear: 0.0015
Total: 5.1523

| Dataset | Number of rows | TC size | Iterations | Time (s) |
| --- | --- | --- | --- | --- |
| cal.cedge | 21693 | 501755 | 195 | 5.1523 |

Benchmark for TG.cedge
----------------------------------------------------------
| Iteration | # Deduplicated union | Join(s) | Deduplication+Projection+Union(s) |
| --- | --- | --- | --- |
| 2 | 0.0009 | 45838 |0.0064 |
| 3 | 0.0009 | 66624 |0.0058 |
| 4 | 0.0010 | 87014 |0.0073 |
| 5 | 0.0015 | 107321 |0.0087 |
| 6 | 0.0009 | 127667 |0.0159 |
| 7 | 0.0016 | 148007 |0.0177 |
| 8 | 0.0009 | 168222 |0.0194 |
| 9 | 0.0009 | 188149 |0.0140 |
| 10 | 0.0009 | 207747 |0.0173 |
| 11 | 0.0009 | 226937 |0.0192 |
| 12 | 0.0009 | 245569 |0.0197 |
| 13 | 0.0009 | 263578 |0.0317 |
| 14 | 0.0009 | 280898 |0.0303 |
| 15 | 0.0010 | 297495 |0.0203 |
| 16 | 0.0009 | 313305 |0.0248 |
| 17 | 0.0023 | 328407 |0.0247 |
| 18 | 0.0029 | 342636 |0.0250 |
| 19 | 0.0032 | 355917 |0.0412 |
| 20 | 0.0041 | 368221 |0.0324 |
| 21 | 0.0027 | 379601 |0.0288 |
| 22 | 0.0023 | 390144 |0.0332 |
| 23 | 0.0036 | 399786 |0.0281 |
| 24 | 0.0016 | 408600 |0.0329 |
| 25 | 0.0031 | 416622 |0.0300 |
| 26 | 0.0025 | 423911 |0.0297 |
| 27 | 0.0028 | 430545 |0.0304 |
| 28 | 0.0016 | 436571 |0.0287 |
| 29 | 0.0016 | 442041 |0.0292 |
| 30 | 0.0021 | 446999 |0.0311 |
| 31 | 0.0009 | 451424 |0.0278 |
| 32 | 0.0009 | 455377 |0.0251 |
| 33 | 0.0017 | 458896 |0.0269 |
| 34 | 0.0009 | 461978 |0.0279 |
| 35 | 0.0009 | 464659 |0.0256 |
| 36 | 0.0025 | 467015 |0.0304 |
| 37 | 0.0009 | 469123 |0.0261 |
| 38 | 0.0009 | 471008 |0.0255 |
| 39 | 0.0009 | 472687 |0.0250 |
| 40 | 0.0015 | 474152 |0.0284 |
| 41 | 0.0009 | 475408 |0.0258 |
| 42 | 0.0019 | 476476 |0.0300 |
| 43 | 0.0009 | 477382 |0.0270 |
| 44 | 0.0009 | 478152 |0.0258 |
| 45 | 0.0009 | 478795 |0.0276 |
| 46 | 0.0009 | 479331 |0.0257 |
| 47 | 0.0017 | 479761 |0.0235 |
| 48 | 0.0017 | 480103 |0.0197 |
| 49 | 0.0016 | 480376 |0.0246 |
| 50 | 0.0016 | 480597 |0.0209 |
| 51 | 0.0016 | 480774 |0.0215 |
| 52 | 0.0015 | 480906 |0.0198 |
| 53 | 0.0015 | 480998 |0.0213 |
| 54 | 0.0014 | 481059 |0.0211 |
| 55 | 0.0014 | 481096 |0.0208 |
| 56 | 0.0014 | 481114 |0.0203 |
| 57 | 0.0014 | 481120 |0.0201 |
| 58 | 0.0013 | 481121 |0.0190 |

Read: 0.0287, reverse: 0.0290
Join: 1.3789
Projection, deduplication, union: 1.3789
Memory clear: 0.0145
Total: 1.5403

| Dataset | Number of rows | TC size | Iterations | Time (s) |
| --- | --- | --- | --- | --- |
| TG.cedge | 23874 | 481121 | 58 | 1.5403 |
Benchmark for OL.cedge
----------------------------------------------------------
| Iteration | # Deduplicated union | Join(s) | Deduplication+Projection+Union(s) |
| --- | --- | --- | --- |
| 2 | 0.0009 | 14319 |0.0031 |
| 3 | 0.0009 | 21813 |0.0035 |
| 4 | 0.0009 | 29352 |0.0031 |
| 5 | 0.0009 | 36851 |0.0036 |
| 6 | 0.0009 | 44235 |0.0032 |
| 7 | 0.0009 | 51441 |0.0036 |
| 8 | 0.0009 | 58424 |0.0041 |
| 9 | 0.0009 | 65147 |0.0037 |
| 10 | 0.0013 | 71596 |0.0042 |
| 11 | 0.0009 | 77694 |0.0036 |
| 12 | 0.0009 | 83457 |0.0037 |
| 13 | 0.0012 | 88894 |0.0042 |
| 14 | 0.0012 | 93984 |0.0052 |
| 15 | 0.0009 | 98738 |0.0045 |
| 16 | 0.0012 | 103161 |0.0042 |
| 17 | 0.0012 | 107252 |0.0059 |
| 18 | 0.0009 | 111015 |0.0045 |
| 19 | 0.0012 | 114460 |0.0059 |
| 20 | 0.0012 | 117619 |0.0059 |
| 21 | 0.0009 | 120475 |0.0049 |
| 22 | 0.0009 | 123011 |0.0045 |
| 23 | 0.0016 | 125308 |0.0054 |
| 24 | 0.0012 | 127436 |0.0052 |
| 25 | 0.0015 | 129399 |0.0062 |
| 26 | 0.0009 | 131199 |0.0065 |
| 27 | 0.0010 | 132828 |0.0077 |
| 28 | 0.0010 | 134275 |0.0075 |
| 29 | 0.0010 | 135536 |0.0096 |
| 30 | 0.0010 | 136633 |0.0075 |
| 31 | 0.0014 | 137589 |0.0082 |
| 32 | 0.0010 | 138429 |0.0079 |
| 33 | 0.0010 | 139178 |0.0082 |
| 34 | 0.0010 | 139847 |0.0079 |
| 35 | 0.0010 | 140452 |0.0080 |
| 36 | 0.0010 | 140995 |0.0079 |
| 37 | 0.0010 | 141481 |0.0080 |
| 38 | 0.0010 | 141924 |0.0074 |
| 39 | 0.0010 | 142327 |0.0072 |
| 40 | 0.0010 | 142694 |0.0073 |
| 41 | 0.0009 | 143034 |0.0079 |
| 42 | 0.0009 | 143360 |0.0079 |
| 43 | 0.0010 | 143675 |0.0080 |
| 44 | 0.0009 | 143976 |0.0079 |
| 45 | 0.0009 | 144264 |0.0083 |
| 46 | 0.0009 | 144532 |0.0087 |
| 47 | 0.0009 | 144779 |0.0086 |
| 48 | 0.0009 | 145002 |0.0086 |
| 49 | 0.0009 | 145198 |0.0086 |
| 50 | 0.0009 | 145369 |0.0086 |
| 51 | 0.0009 | 145518 |0.0085 |
| 52 | 0.0009 | 145645 |0.0086 |
| 53 | 0.0009 | 145755 |0.0085 |
| 54 | 0.0009 | 145847 |0.0086 |
| 55 | 0.0009 | 145922 |0.0085 |
| 56 | 0.0009 | 145980 |0.0085 |
| 57 | 0.0009 | 146024 |0.0085 |
| 58 | 0.0009 | 146057 |0.0084 |
| 59 | 0.0008 | 146082 |0.0087 |
| 60 | 0.0008 | 146098 |0.0086 |
| 61 | 0.0009 | 146109 |0.0085 |
| 62 | 0.0009 | 146116 |0.0084 |
| 63 | 0.0009 | 146119 |0.0084 |
| 64 | 0.0009 | 146120 |0.0084 |

Read: 0.0724, reverse: 0.0726
Join: 0.4334
Projection, deduplication, union: 0.4334
Memory clear: 0.0040
Total: 0.6454

| Dataset | Number of rows | TC size | Iterations | Time (s) |
| --- | --- | --- | --- | --- |
| OL.cedge | 7035 | 146120 | 64 | 0.6454 |
Benchmark for p2p-Gnutella09
----------------------------------------------------------
| Iteration | # Deduplicated union | Join(s) | Deduplication+Projection+Union(s) |
| --- | --- | --- | --- |
| 2 | 0.0022 | 129548 |0.0062 |
| 3 | 0.0052 | 505177 |0.0206 |
| 4 | 0.0057 | 1621937 |0.0246 |
| 5 | 0.0084 | 4262546 |0.0393 |
| 6 | 0.0147 | 8761434 |0.0659 |
| 7 | 0.0234 | 13700045 |0.1079 |
| 8 | 0.0323 | 17258403 |0.1363 |
| 9 | 0.0376 | 19214882 |0.1994 |
| 10 | 0.0641 | 20204059 |0.1944 |
| 11 | 0.0434 | 20751684 |0.1874 |
| 12 | 0.0440 | 21064266 |0.2142 |
| 13 | 0.0440 | 21243127 |0.1910 |
| 14 | 0.0443 | 21339158 |0.1931 |
| 15 | 0.0437 | 21382486 |0.1955 |
| 16 | 0.0514 | 21397410 |0.1921 |
| 17 | 0.0437 | 21401928 |0.1955 |
| 18 | 0.0441 | 21402851 |0.2424 |
| 19 | 0.1158 | 21402957 |0.2379 |
| 20 | 0.1257 | 21402960 |0.2512 |

Read: 0.0387, reverse: 0.0390
Join: 3.1266
Projection, deduplication, union: 3.1266
Memory clear: 0.0222
Total: 4.1448

| Dataset | Number of rows | TC size | Iterations | Time (s) |
| --- | --- | --- | --- | --- |
| p2p-Gnutella09 | 26013 | 21402960 | 20 | 4.1448 |
Benchmark for p2p-Gnutella04
----------------------------------------------------------
| Iteration | # Deduplicated union | Join(s) | Deduplication+Projection+Union(s) |
| --- | --- | --- | --- |
| 2 | 0.0083 | 218370 |0.0318 |
| 3 | 0.0091 | 976616 |0.0375 |
| 4 | 0.0090 | 3842060 |0.0543 |
| 5 | 0.0186 | 11653146 |0.0681 |
| 6 | 0.0304 | 23748922 |0.1369 |
| 7 | 0.1036 | 34088772 |0.3653 |
| 8 | 0.3100 | 40152911 |0.5547 |
| 9 | 0.0781 | 43246096 |0.5311 |
| 10 | 0.2550 | 44812737 |0.6449 |
| 11 | 0.0851 | 45689891 |0.6526 |
| 12 | 0.3733 | 46226844 |0.6409 |
| 13 | 0.1582 | 46539051 |0.6546 |
| 14 | 0.2023 | 46710511 |0.6725 |
| 15 | 0.1662 | 46813927 |0.6597 |
| 16 | 0.3312 | 46883084 |0.5917 |
| 17 | 0.2468 | 46937127 |0.6621 |
| 18 | 0.1804 | 46984995 |0.6188 |
| 19 | 0.2748 | 47023338 |0.6452 |
| 20 | 0.1780 | 47046747 |0.6117 |
| 21 | 0.3964 | 47056798 |0.6285 |
| 22 | 0.1704 | 47059086 |0.6322 |
| 23 | 0.2441 | 47059447 |0.6243 |
| 24 | 0.3161 | 47059508 |0.6264 |
| 25 | 0.1658 | 47059523 |0.7391 |
| 26 | 0.1857 | 47059527 |0.6222 |

Read: 0.0360, reverse: 0.0362
Join: 13.2207
Projection, deduplication, union: 13.2207
Memory clear: 0.0417
Total: 18.1518

| Dataset | Number of rows | TC size | Iterations | Time (s) |
| --- | --- | --- | --- | --- |
| p2p-Gnutella04 | 39994 | 47059527 | 26 | 18.1518 |


## Day 2 Progress
- Generated datasets:
  - 100000 to 1000000, interval 50000

## Comparison between Rapids cudf, pandas df, and nested loop join

### Rapids
- Implemented two versions using `rapids` `cudf` and `pandas` `df`.

| Number of rows | CUDF time (s) | Pandas time (s) |
| --- | --- | --- |
| 100000 | 0.052770 | 0.282879 |
| 150000 | 0.105069 | 0.912774 |

Our nested loop join performance:

| Number of rows | #Blocks | #Threads | #Result rows | Pass 1 | Offset calculation | Pass 2 | Total time |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 100000 | 196 | 512 | 20000986 | 0.0433023 | 0.000323834 | 0.15379 | 0.197416 |
| 100000 | 98 | 1024 | 20000986 | 0.0473048 | 0.000349723 | 0.169904 | 0.217558 |
| 150000 | 293 | 512 | 44995231 | 0.0917667 | 0.000335558 | 0.34609 | 0.438192 |
| 150000 | 147 | 1024 | 44995231 | 0.115846 | 0.00314425 | 0.378974 | 0.497964 |


For cudf we are getting error for `n=200000`:
```
std::bad_alloc: out_of_memory: CUDA error at: /workspace/.conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory
```
- Tried to create conda environment in Theta GPU. Got error:
```shell
(miniconda-3/latest/base) arsho@thetalogin4:~> conda create -p /envs/gpujoin_env --clone $CONDA_PREFIX
Source:      /soft/datascience/conda/miniconda3/latest
Destination: /envs/gpujoin_env
The following packages cannot be cloned out of the root environment:
 - defaults/linux-64::conda-4.8.3-py37_0
Packages: 175
Files: 117276
```

### Cuda
#### Nested loop join dynamic size
- Removed `cudaMemcpy` for host to device and device to host transfer and vice versa
- Added `cudaMallocManaged`
- Removed CPU offset calculation with Thrust's exclusive scan
- Added benchmark
- Gathered report from `nsys` and `nvprof`: 
```shell
nvprof ./join                                
GPU join operation (non-atomic): (500000, 2) x (500000, 2)
Blocks per grid: 489, Threads per block: 1024
==69548== NVPROF is profiling process 69548, command: ./join
GPU Pass 1 get join size per row in relation 1: 1.03881 seconds
Total size of the join result: 1499895627
Thrust calculate offset: 0.000433121 seconds
GPU Pass 2 join operation: 4.41225 seconds
Total time (pass 1 + offset + pass 2): 5.45149
| Number of rows | #Blocks | #Threads | #Result rows | Pass 1 | Offset calculation | Pass 2 | Total time |
| 500000 | 489 | 1024 | 499965209 | 1.03881 | 0.000433121 | 4.41225 | 5.45149 |

==69548== Profiling application: ./join
==69548== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   80.94%  4.41223s         1  4.41223s  4.41223s  4.41223s  gpu_get_join_data_dynamic(int*, int*, int*, int, int, int, int*, int, int, int)
                   19.06%  1.03876s         1  1.03876s  1.03876s  1.03876s  gpu_get_join_size_per_thread(int*, int*, int, int, int, int*, int, int, int)
                    0.00%  29.792us         1  29.792us  29.792us  29.792us  void cub::DeviceScanKernel<cub::DeviceScanPolicy<int>::Policy600, int*, int*, cub::ScanTileState<int, bool=1>, thrust::plus<void>, cub::detail::InputValue<int, int*>, int>(cub::DeviceScanPolicy<int>::Policy600, int*, int*, int, int, bool=1, cub::ScanTileState<int, bool=1>)
                    0.00%  16.831us         1  16.831us  16.831us  16.831us  void cub::DeviceReduceKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600, int*, int*, int, thrust::plus<int>>(int, int, int, cub::GridEvenShare<int>, thrust::plus<int>)
                    0.00%  3.8080us         1  3.8080us  3.8080us  3.8080us  void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600, int*, int*, int, thrust::plus<int>, int>(int, int, int, thrust::plus<int>, cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600)
                    0.00%  2.1120us         1  2.1120us  2.1120us  2.1120us  void cub::DeviceScanInitKernel<cub::ScanTileState<int, bool=1>>(int, int)
                    0.00%     961ns         1     961ns     961ns     961ns  [CUDA memcpy DtoH]
      API calls:   98.14%  5.45101s         2  2.72550s  1.03876s  4.41224s  cudaDeviceSynchronize
                    1.84%  102.19ms         4  25.548ms  13.844us  102.10ms  cudaMallocManaged
                    0.02%  893.35us         5  178.67us  39.549us  396.77us  cudaFree
                    0.00%  166.51us         2  83.257us  57.877us  108.64us  cudaMalloc
                    0.00%  77.436us       101     766ns     105ns  33.159us  cuDeviceGetAttribute
                    0.00%  75.389us         6  12.564us  4.2940us  30.328us  cudaLaunchKernel
                    0.00%  46.990us         3  15.663us  1.0500us  30.646us  cudaStreamSynchronize
                    0.00%  24.301us         1  24.301us  24.301us  24.301us  cudaMemcpyAsync
                    0.00%  21.091us         1  21.091us  21.091us  21.091us  cuDeviceGetName
                    0.00%  12.270us         1  12.270us  12.270us  12.270us  cuDeviceGetPCIBusId
                    0.00%  8.4830us         1  8.4830us  8.4830us  8.4830us  cudaFuncGetAttributes
                    0.00%  6.5520us         3  2.1840us     813ns  4.5290us  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
                    0.00%  5.3840us        54      99ns      91ns     239ns  cudaGetLastError
                    0.00%  4.8910us         9     543ns     212ns  2.0480us  cudaGetDevice
                    0.00%  2.1450us         3     715ns     165ns  1.6500us  cuDeviceGetCount
                    0.00%  2.1080us         3     702ns     396ns  1.1340us  cudaDeviceGetAttribute
                    0.00%     961ns         8     120ns      93ns     190ns  cudaPeekAtLastError
                    0.00%     620ns         1     620ns     620ns     620ns  cuModuleGetLoadingMode
                    0.00%     514ns         2     257ns     141ns     373ns  cuDeviceGet
                    0.00%     333ns         1     333ns     333ns     333ns  cuDeviceTotalMem
                    0.00%     227ns         1     227ns     227ns     227ns  cudaGetDeviceCount
                    0.00%     152ns         1     152ns     152ns     152ns  cuDeviceGetUuid

==69548== Unified Memory profiling result:
Device "NVIDIA GeForce GTX 1060 with Max-Q Design (0)"
   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name
     284  42.873KB  4.0000KB  0.9961MB  11.89063MB  1.132222ms  Host To Device
      85  1.9486MB  4.0000KB  2.0000MB  165.6328MB  13.45680ms  Device To Host
   23837         -         -         -           -  497.1857ms  Gpu page fault groups
Total CPU Page faults: 24
```

```shell
CUDA Kernel Statistics:

 Time (%)  Total Time (ns)  Instances     Avg (ns)         Med (ns)        Min (ns)       Max (ns)     StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  ---------------  ---------------  -------------  -------------  -----------  ----------------------------------------------------------------------------------------------------
     80.8    4,261,112,682          1  4,261,112,682.0  4,261,112,682.0  4,261,112,682  4,261,112,682          0.0  gpu_get_join_data_dynamic(int *, int *, int *, int, int, int, int *, int, int, int)                 
     19.2    1,015,684,940          1  1,015,684,940.0  1,015,684,940.0  1,015,684,940  1,015,684,940          0.0  gpu_get_join_size_per_thread(int *, int *, int, int, int, int *, int, int, int)                     
      0.0           30,592          1         30,592.0         30,592.0         30,592         30,592          0.0  void cub::DeviceScanKernel<cub::DeviceScanPolicy<int>::Policy600, int *, int *, cub::ScanTileState<
      0.0           17,247          1         17,247.0         17,247.0         17,247         17,247          0.0  void cub::DeviceReduceKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::Policy600, 
      0.0            3,839          1          3,839.0          3,839.0          3,839          3,839          0.0  void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<int, int, int, thrust::plus<int>>::P
      0.0            2,080          1          2,080.0          2,080.0          2,080          2,080          0.0  void cub::DeviceScanInitKernel<cub::ScanTileState<int, (bool)1>>(T1, int)                           

[7/8] Executing 'gpumemtimesum' stats report

CUDA Memory Operation Statistics (by time):

 Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)              Operation            
 --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ---------------------------------
     91.0       11,550,832     74  156,092.3  160,223.0       801   198,559     25,850.0  [CUDA Unified Memory memcpy DtoH]
      9.0        1,141,517    297    3,843.5    1,279.0       831    81,664     10,266.5  [CUDA Unified Memory memcpy HtoD]
      0.0              992      1      992.0      992.0       992       992          0.0  [CUDA memcpy DtoH]               

[8/8] Executing 'gpumemsizesum' stats report

CUDA Memory Operation Statistics (by size):

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            
 ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------
    150.610     74     2.035     2.097     0.004     2.097        0.333  [CUDA Unified Memory memcpy DtoH]
     12.468    297     0.042     0.008     0.004     1.044        0.133  [CUDA Unified Memory memcpy HtoD]
      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]    
```

- Run on different configuration of `#threads` and `#blocks`
- Run on `ThetaGpu`:
```shell
qsub -A dist_relational_alg -n 1 -t 15 -q single-gpu --attrs mcdram=flat:filesystems=home -O nested_loop_out join nested_unified
```

| Number of rows | #Blocks | #Threads | #Result rows | Pass 1 | Offset calculation | Pass 2 | Total time |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 100000 | 196 | 512 | 20000986 | 0.00782237 | 0.00151229 | 0.0310354 | 0.04037 |
| 150000 | 293 | 512 | 44995231 | 0.0163467 | 0.00165638 | 0.0682299 | 0.086233 |
| 200000 | 391 | 512 | 80002265 | 0.0276299 | 0.00165044 | 0.143925 | 0.173205 |
| 250000 | 489 | 512 | 125000004 | 0.0425539 | 0.00163754 | 0.169107 | 0.213299 |
| 300000 | 586 | 512 | 179991734 | 0.0595211 | 0.00169447 | 0.225315 | 0.28653 |
| 350000 | 684 | 512 | 245006327 | 0.0798542 | 0.00169882 | 0.283174 | 0.364727 |
| 400000 | 782 | 512 | 319977044 | 0.103554 | 0.00173314 | 0.322186 | 0.427474 |
| 450000 | 879 | 512 | 404982983 | 0.129769 | 0.00179262 | 0.400548 | 0.532109 |
| 500000 | 977 | 512 | 499965209 | 0.158912 | 0.00191387 | 0.485484 | 0.64631 |
| 550000 | 1075 | 512 | 605010431 | 0.168123 | 0.00201475 | 0.558216 | 0.728353 |

Overflow at `n=600000`
```shell
GPU join operation (non-atomic): (600000, 2) x (600000, 2)
Blocks per grid: 1172, Threads per block: 512
GPU Pass 1 get join size per row in relation 1: 0.210211 seconds
Total size of the join result: -2135009041
Thrust calculate offset: 0.00207949 seconds
```
- Tried to add `cuCollections` but getting the following error:
```shell
"C:\Users\ldyke\CUDA\GPUJoin\_deps\libcudacxx-src\include\cuda\std\detail\libcxx\include\support/atomic/atomic_cuda.h( 11): fatal error C1189: #error:  "CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Window s." [C:\Users\ldyke\CUDA\GPUJoin\GPUJoin.vcxproj]" even though I have GPU with >sm_70
```

#### Nested loop join dynamic size atomic
- Added `cudaMallocManaged`.
- Run `nvprof`:
```shell
# thread 8 x 8
nvprof ./join
GPU join operation (atomic): (300000, 2) x (300000, 2)
Block dimension: (37500, 37500, 1), Thread dimension: (8, 8, 1)
==71403== NVPROF is profiling process 71403, command: ./join
Read relations: 0.0418668 seconds
GPU Pass 1 get join size per row in relation 1: 3.85068 seconds
GPU Pass 1 copy result to host: 8.3583e-05 seconds
Total size of the join result: 539975202
GPU Pass 2 join operation: 4.51339 seconds

==71403== Profiling application: ./join
==71403== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   53.96%  4.51335s         1  4.51335s  4.51335s  4.51335s  gpu_get_join_data_dynamic_atomic(int*, int*, int, int*, int, int, int, int*, int, int, int)
                   46.04%  3.85063s         1  3.85063s  3.85063s  3.85063s  gpu_get_total_join_size(int*, int, int*, int, int, int, int*, int, int, int)
                    0.00%  2.5600us         1  2.5600us  2.5600us  2.5600us  [CUDA memcpy HtoD]
                    0.00%  2.1440us         1  2.1440us  2.1440us  2.1440us  [CUDA memcpy DtoH]
      API calls:   97.78%  8.36400s         2  4.18200s  3.85064s  4.51336s  cudaDeviceSynchronize
                    1.43%  121.90ms         5  24.381ms  5.9590us  121.80ms  cudaMallocManaged
                    0.79%  67.433ms         3  22.478ms  140.75us  67.085ms  cudaFree
                    0.00%  209.75us       101  2.0760us     318ns  84.321us  cuDeviceGetAttribute
                    0.00%  97.933us         2  48.966us  16.086us  81.847us  cudaMemcpy
                    0.00%  43.277us         1  43.277us  43.277us  43.277us  cuDeviceGetName
                    0.00%  40.609us         2  20.304us  8.8210us  31.788us  cudaLaunchKernel
                    0.00%  11.987us         1  11.987us  11.987us  11.987us  cuDeviceGetPCIBusId
                    0.00%  3.2030us         3  1.0670us     506ns  2.0800us  cuDeviceGetCount
                    0.00%  1.8310us         2     915ns     317ns  1.5140us  cuDeviceGet
                    0.00%     921ns         1     921ns     921ns     921ns  cuDeviceTotalMem
                    0.00%     636ns         1     636ns     636ns     636ns  cuModuleGetLoadingMode
                    0.00%     584ns         1     584ns     584ns     584ns  cuDeviceGetUuid

==71403== Unified Memory profiling result:
Device "NVIDIA GeForce GTX 1060 with Max-Q Design (0)"
   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name
      36  130.22KB  4.0000KB  0.9961MB  4.578125MB  398.1410us  Host To Device
    6198         -         -         -           -  612.7544ms  Gpu page fault groups
Total CPU Page faults: 18
```
- Runtime error at `n=500000`: `CUDA Runtime Error: out of memory`

### Day 3 progress
- Run benchmark for 10 datasets (100000 - 550000) for Rapids (`cudf`), Pandas (`df`), our CUDA implementations
- Added cudaMemPrefetchAsync for both CUDA implementations
- Run `nsys` for same dataset for cuda implementations to optimize CUDA api calls for different `#blocks` and `#threads`
- Atomic implementation creates lots of barrier synchronization than the non atomic version
